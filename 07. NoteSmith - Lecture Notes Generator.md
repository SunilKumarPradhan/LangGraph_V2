---
title: "NoteSmith - Lecture Notes Generator"
layout: default
nav_order: 6
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes Generator"
last_modified_date: 2026-01-18
source_transcript: "007_Parallel Workflows in LangGraph _ Agentic AI using LangGraph _ Video 6 _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes Generator

# LangGraph Parallel Workflows: Complete Guide from Basics to Advanced

---

## Table of Contents

1. [Overview](#overview)
2. [Recap: What We've Covered So Far](#recap-what-weve-covered-so-far)
3. [Introduction to Parallel Workflows](#introduction-to-parallel-workflows)
4. [Example 1: Cricket Statistics Workflow (Non-LLM)](#example-1-cricket-statistics-workflow-non-llm)
   - [Workflow Design](#workflow-design)
   - [State Definition](#state-definition)
   - [Node Implementation](#node-implementation)
   - [Partial State Updates](#partial-state-updates)
5. [Example 2: UPSC Essay Evaluation (LLM-Based)](#example-2-upsc-essay-evaluation-llm-based)
   - [Workflow Architecture](#workflow-architecture)
   - [Structured Output with Pydantic](#structured-output-with-pydantic)
   - [Reducer Functions](#reducer-functions)
   - [Complete Implementation](#complete-implementation)
6. [Quick Reference](#quick-reference)
7. [Summary Table](#summary-table)
8. [Key Takeaways](#key-takeaways)
9. [Edge Cases & Common Mistakes](#edge-cases--common-mistakes)
10. [Interview Questions](#interview-questions)

---

## Overview

### üìö What This Covers
This lecture focuses on building **parallel workflows** using LangGraph, progressing from simple non-LLM examples to complex LLM-based applications. You'll learn how to execute multiple operations simultaneously, handle state updates correctly, and integrate structured outputs.

### üéØ Prerequisites
- Basic understanding of LangGraph (sequential workflows)
- Familiarity with LangChain concepts
- Python programming fundamentals
- Understanding of LLMs and prompting

### üí° Why It Matters
Parallel workflows are essential for:
- **Performance optimization**: Execute independent tasks simultaneously
- **Real-world applications**: Most production systems require parallel processing
- **Scalability**: Handle multiple operations without sequential bottlenecks
- **Agentic AI**: Build sophisticated AI agents that can multi-task

---

## Recap: What We've Covered So Far

The playlist has covered 5 videos so far:
1. **Conceptual foundations** of Agentic AI
2. **LangGraph fundamentals**
3. **Sequential/Linear workflows** (previous video)
4. **State management basics**
5. **Node and edge concepts**

> **Key Learning from Last Video**: How to build sequential, linear workflows where each node executes one after another.

---

## Introduction to Parallel Workflows

### üîÑ What Are Parallel Workflows?

**Parallel workflows** allow multiple nodes to execute simultaneously rather than sequentially. This is useful when:
- Operations are **independent** of each other
- You want to **reduce execution time**
- Multiple data points need processing **concurrently**

### Sequential vs Parallel

```
SEQUENTIAL:
Start ‚Üí Node A ‚Üí Node B ‚Üí Node C ‚Üí End

PARALLEL:
         ‚îå‚Üí Node A ‚îê
Start ‚Üí  ‚îú‚Üí Node B ‚îú‚Üí Summary ‚Üí End
         ‚îî‚Üí Node C ‚îò
```

### When to Use Parallel Workflows

| Use Parallel When | Use Sequential When |
|------------------|---------------------|
| Tasks are independent | Output of one task feeds into another |
| No data dependencies | Strict order required |
| Performance is critical | Simple logic flow |
| Multiple aspects to evaluate | Single processing pipeline |

---

## Example 1: Cricket Statistics Workflow (Non-LLM)

### üèè Workflow Design

**Goal**: Calculate multiple cricket statistics from a batsman's innings data.

**Input Data**:
- Runs scored
- Balls played
- Number of fours (4s)
- Number of sixes (6s)

**Output Calculations** (all can run in parallel):
1. **Strike Rate**: (Runs / Balls) √ó 100
2. **Boundary Percentage**: (Runs from boundaries / Total runs) √ó 100
3. **Balls per Boundary**: Balls / (Fours + Sixes)

**Workflow Diagram**:
```
                    ‚îå‚Üí Calculate Strike Rate ‚îê
Start (Input Data) ‚Üí‚îú‚Üí Calculate Boundary %  ‚îú‚Üí Summary ‚Üí End
                    ‚îî‚Üí Calculate Balls/Bound ‚îò
```

### State Definition

```python
from typing_extensions import TypedDict

class BatsmanState(TypedDict):
    # Input attributes
    runs: int
    balls: int
    fours: int
    sixes: int
    
    # Calculated attributes
    strike_rate: float
    bpb: float  # Balls per boundary
    boundary_percent: float
    summary: str
```

**Why this structure?**
- **Input attributes**: Data we receive initially
- **Calculated attributes**: Results from parallel nodes
- **Summary**: Final aggregated output

### Node Implementation

#### Node 1: Calculate Strike Rate

```python
def calculate_strike_rate(state: BatsmanState):
    """
    Calculate strike rate: (Runs/Balls) √ó 100
    
    Strike rate shows how quickly a batsman scores.
    Higher is better (aggressive batting).
    """
    strike_rate = (state["runs"] / state["balls"]) * 100
    
    # Return only the calculated value (partial update)
    return {"strike_rate": strike_rate}
```

**Line-by-line breakdown**:
- `state: BatsmanState`: Input is the current state dictionary
- `state["runs"] / state["balls"]`: Runs per ball ratio
- `* 100`: Convert to percentage (runs per 100 balls)
- `return {"strike_rate": strike_rate}`: Return ONLY what changed

#### Node 2: Calculate Balls per Boundary

```python
def calculate_bpb(state: BatsmanState):
    """
    Calculate how frequently batsman hits boundaries.
    Lower is better (more aggressive).
    
    Example: If 10 balls, 4 boundaries ‚Üí 2.5 balls per boundary
    """
    bpb = state["balls"] / (state["fours"] + state["sixes"])
    
    return {"bpb": bpb}
```

**Why this matters**: Shows boundary-hitting frequency, indicating aggressive intent.

#### Node 3: Calculate Boundary Percentage

```python
def calculate_boundary_percent(state: BatsmanState):
    """
    Calculate percentage of runs from boundaries.
    
    Formula: (Runs from boundaries / Total runs) √ó 100
    """
    # Calculate runs from boundaries
    boundary_runs = (state["fours"] * 4) + (state["sixes"] * 6)
    
    # Calculate percentage
    boundary_percent = (boundary_runs / state["runs"]) * 100
    
    return {"boundary_percent": boundary_percent}
```

**Breakdown**:
- `state["fours"] * 4`: Each four = 4 runs
- `state["sixes"] * 6`: Each six = 6 runs
- Division by total runs gives ratio
- Multiply by 100 for percentage

#### Node 4: Summary Node

```python
def summary(state: BatsmanState):
    """
    Aggregate all calculated metrics into readable summary.
    This node runs AFTER all parallel nodes complete.
    """
    summary_text = f"""
    Strike Rate: {state['strike_rate']}
    Balls per Boundary: {state['bpb']}
    Boundary Percent: {state['boundary_percent']}
    """
    
    return {"summary": summary_text}
```

### Building the Graph

```python
from langgraph.graph import StateGraph, START, END

# Create graph with our state
graph = StateGraph(BatsmanState)

# Add all nodes
graph.add_node("calculate_strike_rate", calculate_strike_rate)
graph.add_node("calculate_bpb", calculate_bpb)
graph.add_node("calculate_boundary_percent", calculate_boundary_percent)
graph.add_node("summary", summary)

# Add edges: START to all parallel nodes
graph.add_edge(START, "calculate_strike_rate")
graph.add_edge(START, "calculate_bpb")
graph.add_edge(START, "calculate_boundary_percent")

# Add edges: All parallel nodes to summary
graph.add_edge("calculate_strike_rate", "summary")
graph.add_edge("calculate_bpb", "summary")
graph.add_edge("calculate_boundary_percent", "summary")

# Summary to END
graph.add_edge("summary", END)

# Compile the workflow
workflow = graph.compile()
```

**Edge explanation**:
- **START ‚Üí parallel nodes**: All three calculations start simultaneously
- **Parallel nodes ‚Üí summary**: All must complete before summary runs
- **Summary ‚Üí END**: Final step

### Partial State Updates

#### ‚ùå The Problem (What NOT to do)

```python
def calculate_strike_rate(state: BatsmanState):
    strike_rate = (state["runs"] / state["balls"]) * 100
    state["strike_rate"] = strike_rate
    return state  # ‚ùå WRONG: Returns entire state
```

**Why this fails in parallel workflows**:
When three nodes run in parallel and all return the full state:
- Node A returns: `{runs: 100, balls: 50, strike_rate: 200, ...}`
- Node B returns: `{runs: 100, balls: 50, bpb: 5, ...}`
- Node C returns: `{runs: 100, balls: 50, boundary_percent: 48, ...}`

**Conflict**: LangGraph sees three different values for `runs`, `balls`, etc., and throws `InvalidUpdateError`.

#### ‚úÖ The Solution: Partial Updates

```python
def calculate_strike_rate(state: BatsmanState):
    strike_rate = (state["runs"] / state["balls"]) * 100
    return {"strike_rate": strike_rate}  # ‚úÖ CORRECT: Only what changed
```

**Why this works**:
- Each node returns ONLY the attribute it calculated
- No conflicts because different attributes are being updated
- LangGraph merges all partial updates into the final state

### Running the Workflow

```python
# Define initial state
initial_state = {
    "runs": 100,
    "balls": 50,
    "fours": 6,
    "sixes": 4
}

# Execute workflow
result = workflow.invoke(initial_state)

print(result)
```

**Output**:
```python
{
    'runs': 100,
    'balls': 50,
    'fours': 6,
    'sixes': 4,
    'strike_rate': 200.0,
    'bpb': 5.0,
    'boundary_percent': 48.0,
    'summary': 'Strike Rate: 200.0\nBalls per Boundary: 5.0\nBoundary Percent: 48.0'
}
```

---

## Example 2: UPSC Essay Evaluation (LLM-Based)

### üìù Workflow Architecture

**Goal**: Evaluate UPSC essays on multiple aspects simultaneously using LLMs.

**Evaluation Aspects** (parallel):
1. **Clarity of Thought** (COT)
2. **Depth of Analysis**
3. **Language Quality**

**Each aspect returns**:
- Textual feedback (string)
- Score (0-10)

**Final Node**:
- Summarizes all feedback
- Calculates average score

**Workflow Diagram**:
```
                ‚îå‚Üí Evaluate Language ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   (feedback + score)    ‚îÇ
Start (Essay) ‚Üí‚îú‚Üí Evaluate Analysis ‚îÄ‚îÄ‚îÄ‚îÄ‚îú‚Üí Final Evaluation ‚Üí End
                ‚îÇ   (feedback + score)    ‚îÇ   (summary + avg)
                ‚îî‚Üí Evaluate Clarity ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    (feedback + score)
```

### Structured Output with Pydantic

#### Why Structured Output?

**Problem without structure**:
- LLM might return score as "seven" instead of `7`
- Inconsistent JSON formats
- Missing fields
- Unreliable parsing

**Solution**: Define a schema that LLM MUST follow.

#### Creating the Schema

```python
from pydantic import BaseModel, Field

class EvaluationSchema(BaseModel):
    """
    Schema that enforces structured output from LLM.
    LLM must return data matching this exact structure.
    """
    feedback: str = Field(
        description="Detailed feedback for the essay"
    )
    score: int = Field(
        description="Score out of 10",
        ge=0,  # Greater than or equal to 0
        le=10  # Less than or equal to 10
    )
```

**Field parameters**:
- `description`: Helps LLM understand what to generate
- `ge=0, le=10`: Validation constraints (score must be 0-10)

#### Creating Structured Model

```python
from langchain_openai import ChatOpenAI

# Base model
model = ChatOpenAI(model="gpt-4o-mini")

# Add structured output capability
structured_model = model.with_structured_output(EvaluationSchema)
```

**How it works**:
1. `with_structured_output()` wraps the model
2. Model now MUST return data matching `EvaluationSchema`
3. Automatic validation and parsing

#### Testing Structured Output

```python
essay = """
India's role in AI development is crucial for global technological advancement.
With a large pool of talented engineers and researchers, India has the potential
to become a leader in artificial intelligence innovation...
"""

prompt = f"""
Evaluate the language quality of the following essay and provide a feedback 
and assign a score out of 10.

Essay: {essay}
"""

# Invoke structured model
output = structured_model.invoke(prompt)

# Access structured fields
print(output.feedback)  # String feedback
print(output.score)     # Integer score (guaranteed)
```

**Output**:
```python
EvaluationSchema(
    feedback="The essay demonstrates clear language with good vocabulary...",
    score=8
)
```

### Reducer Functions

#### The Problem

In parallel workflows, when multiple nodes update the **same attribute**, we need to define **how to merge** those updates.

**Scenario**:
```
Node A returns: {"individual_scores": [8]}
Node B returns: {"individual_scores": [7]}
Node C returns: {"individual_scores": [6]}
```

**Without reducer**: Last update overwrites ‚Üí Only `[6]` remains ‚ùå

**With reducer**: All updates merge ‚Üí `[8, 7, 6]` ‚úÖ

#### Implementing Reducer

```python
from typing import Annotated
import operator

class UPSCState(TypedDict):
    essay: str
    language_feedback: str
    analysis_feedback: str
    clarity_feedback: str
    overall_feedback: str
    
    # Reducer function: merge lists using addition
    individual_scores: Annotated[list[int], operator.add]
    
    average_score: float
```

**Breaking down the reducer**:

```python
Annotated[list[int], operator.add]
```

- `list[int]`: Type is a list of integers
- `operator.add`: Reducer function (how to merge)
- `operator.add` is equivalent to `+` operator for lists

**How it works**:
```python
# Three parallel nodes return:
[8] + [7] + [6] = [8, 7, 6]
```

**Why `operator.add`?**
```python
# Cannot do this in type annotation:
individual_scores: list[int] = +  # ‚ùå Syntax error

# Must use functional equivalent:
import operator
operator.add([8], [7])  # Returns [8, 7]
```

**Other reducer options**:
```python
from operator import add, mul
from functools import reduce

# Addition (merge lists)
Annotated[list[int], add]

# Maximum value
Annotated[int, max]

# Minimum value  
Annotated[int, min]

# Custom reducer
def custom_merge(a, b):
    return a + b

Annotated[list[int], custom_merge]
```

### Complete Implementation

#### State Definition

```python
from typing_extensions import TypedDict
from typing import Annotated
import operator

class UPSCState(TypedDict):
    # Input
    essay: str
    
    # Individual feedbacks from parallel nodes
    language_feedback: str
    analysis_feedback: str
    clarity_feedback: str