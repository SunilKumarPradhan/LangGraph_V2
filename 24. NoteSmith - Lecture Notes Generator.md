---
title: "NoteSmith - Lecture Notes Generator"
layout: default
nav_order: 22
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes Generator"
last_modified_date: 2026-01-18
source_transcript: "024_How To Implement Short Term Memory Using LangGraph"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes Generator
# Short-Term Memory Implementation in LangGraph

---

## ğŸ“‘ Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Understanding Memory in LLMs](#understanding-memory-in-llms)
4. [Short-Term Memory Implementation](#short-term-memory-implementation)
5. [Persistence with PostgreSQL](#persistence-with-postgresql)
6. [Context Window Problem](#context-window-problem)
7. [Trimming Technique](#trimming-technique)
8. [Deletion Technique](#deletion-technique)
9. [Summarization Technique](#summarization-technique)
10. [Quick Reference](#quick-reference)
11. [Summary Table](#summary-table)
12. [Key Takeaways](#key-takeaways)
13. [Edge Cases & Common Mistakes](#edge-cases--common-mistakes)
14. [Interview Questions](#interview-questions)

---

## ğŸ¯ Overview

### What This Covers
This comprehensive guide explores **short-term memory implementation in LangGraph**, covering everything from basic concepts to advanced production-ready solutions. You'll learn how to maintain conversation context, persist state across sessions, and handle context window limitations.

### Why It Matters
- **LLMs are stateless** - They don't remember previous interactions by default
- **Production applications** require conversation continuity
- **Context management** is critical for meaningful AI interactions
- **Memory optimization** prevents performance degradation and hallucinations

### Prerequisites
- Basic understanding of LLMs and their limitations
- Familiarity with Python programming
- Knowledge of LangGraph fundamentals
- Understanding of conversation buffers (covered in previous video)

### What You'll Learn
1. How to implement short-term memory using checkpointers and thread IDs
2. Persisting conversation state using PostgreSQL
3. Solving context overflow with trimming, deletion, and summarization
4. Production-ready memory management patterns

---

## ğŸ§  Understanding Memory in LLMs

### The Fundamental Problem

> **Key Concept**: LLMs have no intrinsic memory. Every API call is treated as a fresh, independent conversation with no knowledge of previous interactions.

#### Why LLMs Don't Remember

```python
# Each invocation is stateless
response1 = llm.invoke("My name is Nitesh")
response2 = llm.invoke("What is my name?")  
# âŒ LLM doesn't know - no memory of previous call
```

**The Challenge**: 
- LLMs process input â†’ generate output â†’ forget everything
- No built-in mechanism to retain conversation history
- Each `llm.invoke()` call is independent

### The Solution: Conversation Buffer

**Concept**: Maintain a buffer that stores all messages and sends the entire conversation history with each new message.

```python
# Conversation buffer approach
conversation_history = [
    {"role": "user", "content": "My name is Nitesh"},
    {"role": "assistant", "content": "Nice to meet you, Nitesh!"},
    {"role": "user", "content": "What is my name?"}
]

# Send entire history with each call
response = llm.invoke(conversation_history)
# âœ… LLM now has context and can answer correctly
```

**How It Works**:
1. Store every user message and AI response
2. When user sends new message, concatenate it with entire history
3. Send combined context to LLM
4. LLM processes full conversation and responds appropriately
5. Add new response to history buffer

This is the foundation of **short-term memory** in LLMs.

---

## ğŸ”§ Short-Term Memory Implementation

### Core Components

LangGraph provides two key mechanisms for implementing short-term memory:

1. **Checkpointers** - Store graph state at every superstep
2. **Thread IDs** - Identify and isolate individual conversations

### Understanding Checkpointers

**What is a Checkpointer?**
A checkpointer saves your graph's state at each step, enabling:
- Conversation continuity across multiple invocations
- State retrieval and restoration
- Thread-based conversation isolation

**Types of Checkpointers**:
- `MemorySaver` - Stores state in RAM (volatile, for development)
- `PostgresSaver` - Stores state in database (persistent, for production)

### Understanding Thread IDs

**Thread ID** = Unique identifier for a conversation session

```python
# Different threads = Different conversations
config_thread1 = {"configurable": {"thread_id": "thread_1"}}
config_thread2 = {"configurable": {"thread_id": "thread_2"}}

# Each thread maintains separate conversation history
```

---

## ğŸ’» Basic Implementation Code

### Step 1: Import Required Libraries

```python
from langgraph.graph import StateGraph, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# Load environment variables (API keys)
load_dotenv()
```

**Explanation**:
- `StateGraph` - Creates the graph structure
- `MessagesState` - Built-in state that manages message lists
- `MemorySaver` - In-memory checkpointer for state storage
- `ChatOpenAI` - OpenAI LLM wrapper

### Step 2: Initialize the Model

```python
# Create LLM instance
model = ChatOpenAI(model="gpt-3.5-turbo")
```

### Step 3: Define Graph Node

```python
def call_model(state: MessagesState):
    """
    Node that calls the LLM with current conversation state
    
    Args:
        state: Contains 'messages' list with conversation history
    
    Returns:
        Dictionary with AI response message
    """
    # Extract messages from state
    messages = state["messages"]
    
    # Invoke LLM with all messages (conversation context)
    response = model.invoke(messages)
    
    # Return response to be added to state
    return {"messages": [response]}
```

**Line-by-line breakdown**:
- `state["messages"]` - Retrieves all previous messages from state
- `model.invoke(messages)` - Sends entire conversation to LLM
- Returns dictionary that LangGraph automatically merges into state

### Step 4: Build the Graph

```python
# Create graph builder with MessagesState
builder = StateGraph(MessagesState)

# Add the chat node
builder.add_node("chat", call_model)

# Define edges: START -> chat -> END
builder.add_edge("__start__", "chat")
builder.add_edge("chat", "__end__")

# Compile WITHOUT checkpointer (no memory yet)
graph = builder.compile()
```

### Step 5: Test Without Memory

```python
# First invocation
response1 = graph.invoke({
    "messages": [{"role": "user", "content": "Hi, my name is Nitesh"}]
})

# Second invocation
response2 = graph.invoke({
    "messages": [{"role": "user", "content": "What is my name?"}]
})

print(response2)
# Output: "I'm sorry, I cannot know your name as I am a computer program"
# âŒ No memory - LLM doesn't remember previous message
```

---

## âœ… Adding Short-Term Memory

### Step 6: Create Checkpointer

```python
from langgraph.checkpoint.memory import MemorySaver

# Create in-memory checkpointer
checkpointer = MemorySaver()
```

### Step 7: Compile with Checkpointer

```python
# Rebuild graph (same structure)
builder = StateGraph(MessagesState)
builder.add_node("chat", call_model)
builder.add_edge("__start__", "chat")
builder.add_edge("chat", "__end__")

# â­ KEY DIFFERENCE: Add checkpointer during compilation
graph = builder.compile(checkpointer=checkpointer)
```

**What changed?**: The `checkpointer=checkpointer` parameter enables state persistence.

### Step 8: Use Thread IDs

```python
# Create configuration with thread ID
config = {"configurable": {"thread_id": "thread_1"}}

# First message with thread context
response1 = graph.invoke(
    {"messages": [{"role": "user", "content": "Hi, my name is Nitesh"}]},
    config=config  # â­ Pass thread ID
)

# Second message in SAME thread
response2 = graph.invoke(
    {"messages": [{"role": "user", "content": "What is my name?"}]},
    config=config  # â­ Same thread ID
)

print(response2)
# Output: "Your name is Nitesh"
# âœ… Memory works! LLM remembers from previous message
```

### Step 9: Verify State Storage

```python
# Retrieve current state for thread_1
current_state = graph.get_state(config)

# View all messages in this thread
print(current_state.values["messages"])
# Shows all 4 messages:
# 1. User: "Hi, my name is Nitesh"
# 2. AI: "Nice to meet you, Nitesh!"
# 3. User: "What is my name?"
# 4. AI: "Your name is Nitesh"
```

### Step 10: Test Thread Isolation

```python
# Create DIFFERENT thread
config_2 = {"configurable": {"thread_id": "thread_2"}}

# Ask same question in new thread
response3 = graph.invoke(
    {"messages": [{"role": "user", "content": "What is my name?"}]},
    config=config_2  # Different thread
)

print(response3)
# Output: "I'm sorry, I don't have the ability to know your name"
# âœ… Correct! New thread has no previous context
```

**Why this works**:
- Each thread maintains separate conversation history
- `thread_1` has 4 messages stored
- `thread_2` has only 1 message (the question)
- Threads are completely isolated from each other

---

## ğŸ—„ï¸ Persistence with PostgreSQL

### The Problem with MemorySaver

```python
# After program restart
current_state = graph.get_state(config)
print(current_state.values["messages"])
# Output: [] (empty)
# âŒ All conversation history lost!
```

**Why?**
- `MemorySaver` stores state in **RAM** (volatile memory)
- When program terminates, RAM is cleared
- All conversation history disappears
- Not suitable for production applications

### The Solution: PostgreSQL Database

**Production Setup**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangGraph  â”‚
â”‚ Application â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL â”‚ â—„â”€â”€ Persistent storage
â”‚  Database   â”‚     Survives restarts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ³ PostgreSQL Setup with Docker

### Step 1: Install Docker

1. Download from: https://www.docker.com/products/docker-desktop
2. Install Docker Desktop
3. Start Docker Desktop application

### Step 2: Verify Docker Installation

```bash
# Check Docker version
docker --version
# Output: Docker version 24.0.x, build xxxxx
```

### Step 3: Create docker-compose.yml

```yaml
# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:15  # PostgreSQL version 15
    environment:
      POSTGRES_USER: langraph_user      # Database username
      POSTGRES_PASSWORD: langraph_pass  # Database password
      POSTGRES_DB: langraph_db          # Database name
    ports:
      - "5432:5432"  # Port mapping: host:container
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

**Explanation**:
- `image: postgres:15` - Downloads PostgreSQL 15 Docker image
- `environment` - Sets database credentials
- `ports` - Maps container port 5432 to host port 5432
- `volumes` - Persists data even when container stops

### Step 4: Start PostgreSQL Container

```bash
# Run in terminal from project directory
docker-compose up -d

# Verify container is running
docker ps
# Should show postgres container with status "Up"
```

### Step 5: Install Python Dependencies

```bash
pip install langgraph
pip install langgraph-checkpoint-postgres
pip install psycopg
pip install langchain-openai
```

**What each package does**:
- `langgraph` - Core LangGraph framework
- `langgraph-checkpoint-postgres` - PostgreSQL checkpointer
- `psycopg` - PostgreSQL database adapter
- `langchain-openai` - OpenAI integration

---

## ğŸ’¾ PostgreSQL Implementation Code

### Complete Working Example

```python
from langgraph.graph import StateGraph, MessagesState
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize LLM
model = ChatOpenAI()

# Define chat node (same as before)
def call_model(state: MessagesState):
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}

# Database connection string
DB_URI = "postgresql://langraph_user:langraph_pass@localhost:5432/langraph_db"
#         â””â”€protocolâ”€â”˜ â””â”€userâ”€â”˜ â””â”€passwordâ”€â”˜ â””â”€hostâ”€â”˜â””portâ”˜â””databaseâ”˜

# â­ KEY: Use context manager for PostgresSaver
with PostgresSaver.from_connection_string(DB_URI) as checkpointer:
    # Setup database tables (first time only)
    checkpointer.setup()
    
    # Build graph
    builder = StateGraph(MessagesState)
    builder.add_node("chat", call_model)
    builder.add_edge("__start__", "chat")
    builder.add_edge("chat", "__end__")
    
    # Compile with PostgreSQL checkpointer
    graph = builder.compile(checkpointer=checkpointer)
    
    # Create thread configuration
    config = {"configurable": {"thread_id": "thread_1"}}
    
    # First conversation
    graph.invoke(
        {"messages": [{"role": "user", "content": "Hi, my name is Nitesh"}]},
        config=config
    )
    
    # Second message
    response = graph.invoke(
        {"messages": [{"role": "user", "content": "What is my name?"}]},
        config=config
    )
    
    print(response)
    # Output: "Your name is Nitesh"
```

### Testing Persistence

```python
# RESTART your Python kernel/program here
# Then run this code:

from langgraph.checkpoint.postgres import PostgresSaver

DB_URI = "postgresql://langraph_user:langraph_pass@localhost:5432/langraph_db"

with PostgresSaver.from_connection_string(DB_URI) as checkpointer:
    # Rebuild graph (no need to setup again)
    builder = StateGraph(MessagesState)
    builder.add_node("chat", call_model)
    builder.add_edge("__start__", "chat")
    builder.add_edge("chat", "__end__")
    graph = builder.compile(checkpointer=checkpointer)
    
    # Retrieve state from BEFORE restart
    config = {"configurable": {"thread_id": "thread_1"}}
    state = graph.get_state(config)
    
    print(state.values["messages"])
    # âœ… Shows all previous messages!
    # Conversation survived program restart!
```

**Why this works**:
- PostgreSQL stores data on disk (persistent)
- Data survives program restarts, system reboots
- Production-ready solution
- Can handle millions of conversations

---

## âš ï¸ Context Window Problem

### Understanding Context Windows

**Context Window** = Maximum number of tokens an LLM can process in a single request

| Model | Context Window |
|-------|----------------|
| GPT-3.5-turbo | 4,096 tokens |
| GPT-4 | 8,192 tokens |
| GPT-4-turbo | 128,000 tokens |
| Claude 3 | 200,000 tokens |

### The Problem with Short-Term Memory

```python
# Conversation