---
title: "NoteSmith - Lecture Notes Generator"
layout: default
nav_order: 11
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes Generator"
last_modified_date: 2026-01-18
source_transcript: "012_Building a Chatbot with UI in LangGraph & Streamlit _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes Generator

# Building a Chatbot UI with Streamlit and LangGraph

---

## üìë Table of Contents

1. [Overview](#-overview)
2. [Prerequisites](#-prerequisites)
3. [Understanding the Architecture](#-understanding-the-architecture)
4. [Streamlit Fundamentals for Chat Interfaces](#-streamlit-fundamentals-for-chat-interfaces)
5. [Building the Backend with LangGraph](#-building-the-backend-with-langgraph)
6. [Building the Frontend with Streamlit](#-building-the-frontend-with-streamlit)
7. [Integrating Backend and Frontend](#-integrating-backend-and-frontend)
8. [Session State Management](#-session-state-management)
9. [Complete Implementation](#-complete-implementation)
10. [Quick Reference](#-quick-reference)
11. [Summary Table](#-summary-table)
12. [Key Takeaways](#-key-takeaways)
13. [Edge Cases & Common Mistakes](#-edge-cases--common-mistakes)
14. [Interview Questions](#-interview-questions)

---

## üéØ Overview

### What This Covers

This comprehensive guide teaches you how to build a **production-ready chatbot with a web interface** by combining:
- **LangGraph** for the AI backend (conversation logic and memory)
- **Streamlit** for the frontend (user interface)

### Why It Matters

Previously, chatbots built with LangGraph could only be tested in Jupyter notebooks. This tutorial bridges that gap by:
- Creating a clean, professional web interface
- Enabling real users to interact with your AI chatbot
- Maintaining conversation history across interactions
- Providing a foundation for deploying AI applications

### Prerequisites

1. **Streamlit Basics**: Understanding of Streamlit fundamentals (15-20 minute tutorial recommended)
2. **LangGraph Knowledge**: Familiarity with LangGraph workflows, nodes, and graphs
3. **Python Fundamentals**: Basic Python programming skills
4. **Previous Tutorial**: Knowledge from the "LangGraph chatbot with short-term memory" tutorial

### What You'll Build

A fully functional chatbot with:
- Clean, scrollable chat interface
- User and AI message differentiation (with icons)
- Persistent conversation history within a session
- Real-time AI responses using LangGraph backend
- Professional UI/UX design

---

## üèóÔ∏è Understanding the Architecture

### Two-Component System

Modern chatbots require separation of concerns:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   USER                          ‚îÇ
‚îÇ                     ‚Üì                           ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ              ‚îÇ   FRONTEND   ‚îÇ                   ‚îÇ
‚îÇ              ‚îÇ  (Streamlit) ‚îÇ                   ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                     ‚Üì                           ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ              ‚îÇ   BACKEND    ‚îÇ                   ‚îÇ
‚îÇ              ‚îÇ  (LangGraph) ‚îÇ                   ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Breakdown

| Component | Technology | Responsibility |
|-----------|------------|----------------|
| **Backend** | LangGraph | AI logic, conversation flow, memory management |
| **Frontend** | Streamlit | User interface, message display, input handling |

### Why This Separation?

> **Separation of Concerns**: Backend handles "what to say" (AI logic), Frontend handles "how to show it" (presentation)

**Benefits:**
- **Maintainability**: Change UI without touching AI logic
- **Scalability**: Can swap Streamlit for React/Vue later
- **Testing**: Test backend and frontend independently
- **Reusability**: Same backend can serve multiple frontends (web, mobile, API)

### Data Flow

```
1. User types message in Streamlit UI
2. Frontend sends message to LangGraph backend
3. Backend processes with LLM and returns response
4. Frontend displays response in chat interface
5. Both messages stored in session history
```

---

## üé® Streamlit Fundamentals for Chat Interfaces

### Core Components

Streamlit provides two essential components for chat interfaces:

#### 1. **Chat Message** (`st.chat_message`)

Displays individual messages with role-based styling.

```python
import streamlit as st

# Display user message
with st.chat_message("user"):
    st.text("Hello!")

# Display AI assistant message
with st.chat_message("assistant"):
    st.text("How can I help you?")
```

**Parameters:**
- `role`: Either `"user"` or `"assistant"` (determines icon and styling)
- `avatar`: (Optional) Custom avatar image

**Output:**
- User messages get a user icon (üë§)
- Assistant messages get a robot icon (ü§ñ)

#### 2. **Chat Input** (`st.chat_input`)

Creates an input box for users to type messages.

```python
# Create input box
user_input = st.chat_input(placeholder="Type here...")

# user_input contains the text when user presses Enter
if user_input:
    st.write(f"You said: {user_input}")
```

**Key Behavior:**
- Returns `None` until user presses Enter
- Returns the typed text when Enter is pressed
- Automatically clears after submission

### Building Your First Chat Interface

#### Step 1: Display Static Messages

```python
import streamlit as st

# User message
with st.chat_message("user"):
    st.text("Hi")

# Assistant message
with st.chat_message("assistant"):
    st.text("Hello! How can I assist you today?")

# Another user message
with st.chat_message("user"):
    st.text("My name is Nitesh")
```

**Run with:**
```bash
streamlit run streamlit_frontend.py
```

#### Step 2: Add Input Box

```python
import streamlit as st

# Input box at bottom
user_input = st.chat_input(placeholder="Type here")

# Display user's message when they type
if user_input:
    with st.chat_message("user"):
        st.text(user_input)
```

**What happens:**
- User types "Hello" and presses Enter
- Message appears in a user chat bubble
- Input box clears automatically

#### Step 3: Echo Bot (Copy-Cat Chatbot)

```python
import streamlit as st

# Get user input
user_input = st.chat_input(placeholder="Type here")

# When user sends a message
if user_input:
    # Display user message
    with st.chat_message("user"):
        st.text(user_input)
    
    # Display same message as assistant (echo)
    with st.chat_message("assistant"):
        st.text(user_input)  # Just echoes back
```

**Problem with this approach:**
- Previous messages disappear when you send a new one
- No conversation history maintained

**Why?** Streamlit reruns the entire script on every interaction, resetting all variables.

---

## üß† Session State Management

### The Problem: Script Reruns

Every time a user interacts with Streamlit (clicks, types, presses Enter), **the entire script runs from top to bottom again**.

```python
# This gets reset to empty list on every rerun!
message_history = []

user_input = st.chat_input("Type here")
if user_input:
    message_history.append({"role": "user", "content": user_input})
    # message_history is empty again on next rerun!
```

### The Solution: Session State

> **Session State** is a special dictionary that persists across reruns. Data stored here survives until the page is manually refreshed.

#### Basic Usage

```python
import streamlit as st

# Initialize session state variable (only runs once)
if "message_history" not in st.session_state:
    st.session_state.message_history = []

# Now this list persists across reruns!
st.session_state.message_history.append({"role": "user", "content": "Hi"})
```

#### How It Works

```
First Run:
- Check if "message_history" exists in session_state ‚Üí NO
- Create empty list: st.session_state.message_history = []

Second Run (after user interaction):
- Check if "message_history" exists in session_state ‚Üí YES
- Skip initialization, use existing list
- List still contains previous messages!
```

### Message History Structure

We store messages as a list of dictionaries:

```python
st.session_state.message_history = [
    {"role": "user", "content": "Hi"},
    {"role": "assistant", "content": "Hello! How can I help?"},
    {"role": "user", "content": "What's the capital of India?"},
    {"role": "assistant", "content": "The capital of India is New Delhi."}
]
```

**Why this structure?**
- `role`: Identifies who sent the message (for proper icon display)
- `content`: The actual message text
- List maintains chronological order

---

## üîß Building the Backend with LangGraph

### Project Setup

Create a new project folder with two Python files:

```
chatbot_project/
‚îú‚îÄ‚îÄ langgraph_backend.py    # AI logic
‚îú‚îÄ‚îÄ streamlit_frontend.py   # UI
‚îî‚îÄ‚îÄ .env                    # API keys
```

### Backend Code (`langgraph_backend.py`)

This is the **exact same code** from the previous LangGraph tutorial:

```python
# langgraph_backend.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver

# Load environment variables
load_dotenv()

# Initialize LLM
llm = ChatOpenAI(model="gpt-4", api_key=os.getenv("OPENAI_API_KEY"))

# Define the chat node
def chat_node(state: MessagesState):
    """Process messages and return AI response"""
    return {"messages": llm.invoke(state["messages"])}

# Build the graph
graph_builder = StateGraph(MessagesState)
graph_builder.add_node("chat", chat_node)
graph_builder.add_edge(START, "chat")
graph_builder.add_edge("chat", END)

# Add memory (checkpointer)
memory = MemorySaver()
chatbot = graph_builder.compile(checkpointer=memory)
```

### Understanding the Backend

#### 1. **State Definition**

```python
from langgraph.graph import MessagesState
```

`MessagesState` is a built-in state that contains:
- `messages`: List of all conversation messages

#### 2. **Chat Node**

```python
def chat_node(state: MessagesState):
    return {"messages": llm.invoke(state["messages"])}
```

**What it does:**
- Takes current conversation state
- Sends all messages to LLM
- Returns LLM's response
- LangGraph automatically appends response to message list

#### 3. **Graph Structure**

```python
graph_builder.add_edge(START, "chat")  # Start ‚Üí Chat Node
graph_builder.add_edge("chat", END)    # Chat Node ‚Üí End
```

**Flow:**
```
START ‚Üí chat_node (LLM processes) ‚Üí END
```

#### 4. **Memory Saver (Checkpointer)**

```python
memory = MemorySaver()
chatbot = graph_builder.compile(checkpointer=memory)
```

**Why needed?**
- Stores conversation history across multiple invocations
- Requires a `thread_id` to identify different conversations
- Enables the chatbot to "remember" previous messages

### Installing Dependencies

```bash
pip install langchain langchain-openai langgraph streamlit python-dotenv
```

### Environment Setup (`.env`)

```
OPENAI_API_KEY=your_openai_api_key_here
```

---

## üíª Building the Frontend with Streamlit

### Step-by-Step Implementation

#### Version 1: Basic Echo Bot with History

```python
# streamlit_frontend.py
import streamlit as st

# Initialize session state for message history
if "message_history" not in st.session_state:
    st.session_state.message_history = []

# Display all previous messages
for message in st.session_state.message_history:
    with st.chat_message(message["role"]):
        st.text(message["content"])

# Get user input
user_input = st.chat_input(placeholder="Type here")

# Process new message
if user_input:
    # Add user message to history
    st.session_state.message_history.append({
        "role": "user",
        "content": user_input
    })
    
    # Display user message
    with st.chat_message("user"):
        st.text(user_input)
    
    # Add assistant message to history (echo for now)
    st.session_state.message_history.append({
        "role": "assistant",
        "content": user_input  # Just echoing back
    })
    
    # Display assistant message
    with st.chat_message("assistant"):
        st.text(user_input)
```

**Run it:**
```bash
streamlit run streamlit_frontend.py
```

**How it works:**

1. **First run:**
   - `message_history` doesn't exist ‚Üí create empty list
   - No messages to display
   - Show input box

2. **User types "Hi" and presses Enter:**
   - Script reruns from top
   - `message_history` exists ‚Üí skip initialization
   - Loop through history (empty) ‚Üí nothing displayed yet
   - `user_input` = "Hi"
   - Add to history: `{"role": "user", "content": "Hi"}`
   - Display user message
   - Add to history: `{"role": "assistant", "content": "Hi"}`
   - Display assistant message

3. **User types "Hello":**
   - Script reruns
   - Loop displays previous messages (Hi/Hi)
   - Add new messages (Hello/Hello)
   - Now shows: Hi, Hi, Hello, Hello

### Code Breakdown: Line by Line

```python
# Line 1-2: Import Streamlit
import streamlit as st

# Line 4-6: Initialize message history ONCE
# This only runs the first time, then skips on reruns
if "message_history" not in st.session_state:
    st.session_state.message_history = []

# Line 8-11: Display conversation history
# Loops through all stored messages and displays them
for message in st.session_state.message_history:
    with st.chat_message(message["role"]):  # "user" or "assistant"
        st.text(message["content"])  # The actual message text

# Line 13-14: Create input box
# Returns None until user presses Enter, then returns the text
user_input = st.chat_input(placeholder="Type here")

# Line 16-17: Check if user sent a message
if user_input:
    # Line 18-21: Store user message
    st.session_state.message_history.append({
        "role": "user",
        "content": user_input
    })
    
    # Line 23-25: Display user message immediately
    with st.chat_message("user"):
        st.text(user_input)
    
    # Line 27-30: Store assistant message (echo)
    st.session_state.message_history.append({
        "role": "assistant",
        "content": user_input  # Echoing for now
    })
    
    # Line 32-34: Display assistant message
    with st.chat_message("assistant"):
        st.text(user_input)
```

---

## üîó Integrating Backend and Frontend

### The Integration Strategy

We need to replace the "echo" logic with actual AI responses from LangGraph.

**What changes:**
- ‚ùå Remove: `content: user_input` (echo)
- ‚úÖ Add: Call LangGraph backend and get AI response

### Step 1: Import the Chatbot

```python
# streamlit_frontend.py
import streamlit as st
from langchain_core.messages import HumanMessage
from langgraph_backend import chatbot  # Import our LangGraph chatbot
```

**What we're importing:**
- `chatbot`: The compiled LangGraph object with `.invoke()` method
- `HumanMessage`: LangChain's message format for user inputs

### Step 2: Define Configuration

```python
# Configuration for thread management
CONFIG = {
    "configurable