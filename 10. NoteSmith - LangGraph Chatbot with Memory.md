---
title: "NoteSmith - LangGraph Chatbot with Memory"
layout: default
nav_order: 9
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - LangGraph Chatbot with Memory"
last_modified_date: 2026-01-18
source_transcript: "010_How to build a Chatbot using LangGraph"
generated_by: "NoteSmith"
---

# NoteSmith - LangGraph Chatbot with Memory

## Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Understanding Chatbot Architecture](#understanding-chatbot-architecture)
4. [State Management in LangGraph](#state-management-in-langgraph)
5. [Building the Basic Chatbot](#building-the-basic-chatbot)
6. [The Memory Problem](#the-memory-problem)
7. [Implementing Persistence](#implementing-persistence)
8. [Complete Working Code](#complete-working-code)
9. [Quick Reference](#quick-reference)
10. [Summary Table](#summary-table)
11. [Key Takeaways](#key-takeaways)
12. [Common Mistakes & Edge Cases](#common-mistakes--edge-cases)
13. [Interview Questions](#interview-questions)

---

## Overview

### ğŸ“š What This Covers

This lecture teaches you how to build a **conversational chatbot using LangGraph** that maintains conversation history (memory). This is the foundation for building advanced agentic AI applications.

**What you'll build:**
- A basic LLM-based chatbot
- Conversation memory implementation
- Persistent state management using checkpointers

**Future additions (upcoming videos):**
- RAG (Retrieval Augmented Generation)
- Tool integration
- UI development
- LangSmith integration
- Human-in-the-loop (HITL)
- Retry logic and fault tolerance

### ğŸ¯ Prerequisites

- Basic Python knowledge
- Understanding of LangGraph fundamentals (state graphs, nodes, edges)
- Familiarity with workflow types: sequential, parallel, conditional, looping
- LangChain message types (HumanMessage, AIMessage, SystemMessage)
- OpenAI API key

### ğŸ’¡ Why This Matters

Real-world chatbots need to:
- Remember previous conversations
- Maintain context across multiple interactions
- Handle multiple users simultaneously
- Persist data beyond program execution

This is essential for customer support bots, AI assistants, and any conversational AI application.

---

## Understanding Chatbot Architecture

### ğŸ—ï¸ Chatbot Design

A chatbot is essentially an **LLM-based sequential workflow** with a single node.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CHAT NODE     â”‚
â”‚  (LLM Process)  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   END   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flow:**
1. **User Input**: "What is the capital of India?"
2. **Chat Node**: LLM processes the message
3. **LLM Response**: "New Delhi"
4. **Loop**: Repeat until user exits

### ğŸ“Š State Structure

> **State** in a chatbot is the collection of all messages exchanged between user and LLM.

**State contains:**
- User messages (HumanMessage)
- AI responses (AIMessage)
- System prompts (SystemMessage)
- Tool messages (ToolMessage)

**Example conversation state:**
```python
messages = [
    "What is the capital of India?",      # User
    "New Delhi",                          # AI
    "What is the capital of Portugal?",   # User
    "Lisbon"                              # AI
]
```

---

## State Management in LangGraph

### ğŸ”§ Defining State

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage

class ChatState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
```

**Line-by-line breakdown:**

1. **`from typing import Annotated`**: Import for type hints with metadata
2. **`from typing_extensions import TypedDict`**: Typed dictionary support
3. **`from langgraph.graph.message import add_messages`**: Reducer function for appending messages
4. **`from langchain_core.messages import BaseMessage`**: Base class for all message types

5. **`class ChatState(TypedDict)`**: Define state as a typed dictionary
6. **`messages: Annotated[list[BaseMessage], add_messages]`**: 
   - `list[BaseMessage]`: List that can contain HumanMessage, AIMessage, SystemMessage, ToolMessage
   - `add_messages`: Reducer function that **appends** new messages instead of replacing

### ğŸ¤” Why Use a Reducer Function?

**Problem without reducer:**
```python
# First message
state = {"messages": ["What is the capital of India?"]}

# Second message - REPLACES first message âŒ
state = {"messages": ["New Delhi"]}  
```

**Solution with `add_messages` reducer:**
```python
# First message
state = {"messages": ["What is the capital of India?"]}

# Second message - APPENDS to list âœ…
state = {"messages": ["What is the capital of India?", "New Delhi"]}
```

### ğŸ“ Message Types Explained

| Message Type | Purpose | Example |
|-------------|---------|---------|
| **HumanMessage** | User input | "What is the capital of India?" |
| **AIMessage** | LLM response | "New Delhi" |
| **SystemMessage** | Role definition | "You are an experienced data scientist" |
| **ToolMessage** | Tool execution results | Function call outputs |

All inherit from **BaseMessage**, providing flexibility.

---

## Building the Basic Chatbot

### Step 1: Import Dependencies

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI
```

### Step 2: Define State

```python
class ChatState(TypedDict):
    """
    State schema for chatbot
    - messages: List of all conversation messages
    - Uses add_messages reducer to append instead of replace
    """
    messages: Annotated[list[BaseMessage], add_messages]
```

### Step 3: Create the Chat Node Function

```python
def chat_node(state: ChatState) -> dict:
    """
    Core chatbot logic
    
    Args:
        state: Current conversation state with message history
        
    Returns:
        Dictionary with new AI message to add to state
    """
    # Initialize LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    
    # Extract all messages from state
    messages = state["messages"]
    
    # Send messages to LLM and get response
    response = llm.invoke(messages)
    
    # Return response wrapped in list for reducer function
    return {"messages": [response]}
```

**Why return `[response]` in a list?**
- State expects `list[BaseMessage]`
- The `add_messages` reducer merges this list with existing messages

### Step 4: Build the Graph

```python
# Create state graph with ChatState schema
graph = StateGraph(ChatState)

# Add the chat node
graph.add_node("chat_node", chat_node)

# Define edges: START -> chat_node -> END
graph.add_edge("__start__", "chat_node")
graph.add_edge("chat_node", "__end__")

# Compile the graph
chatbot = graph.compile()
```

### Step 5: Test Basic Functionality

```python
# Create initial state with user message
initial_state = {
    "messages": [
        HumanMessage(content="What is the capital of India?")
    ]
}

# Invoke chatbot
response = chatbot.invoke(initial_state)

# Extract AI response
ai_response = response["messages"][-1].content
print(ai_response)  # Output: "The capital of India is New Delhi"
```

**Visualize the graph:**
```python
from IPython.display import Image, display
display(Image(chatbot.get_graph().draw_mermaid_png()))
```

---

## The Memory Problem

### ğŸš¨ Issue: Chatbot Forgets Everything

**Problem demonstration:**

```python
# First interaction
initial_state = {
    "messages": [HumanMessage(content="Hi, my name is Nitesh")]
}
response = chatbot.invoke(initial_state)
# AI: "Hello Nitesh, how can I assist you?"

# Second interaction - NEW invoke call
initial_state = {
    "messages": [HumanMessage(content="What is my name?")]
}
response = chatbot.invoke(initial_state)
# AI: "I'm sorry, I don't have access to your personal information"
```

### ğŸ” Why This Happens

**Root cause:** Each `invoke()` call starts with a **fresh state**

```python
while True:
    user_message = input("Type here: ")
    
    # Problem: Each invoke creates NEW state from scratch
    response = chatbot.invoke({
        "messages": [HumanMessage(content=user_message)]
    })
    # Previous messages are LOST âŒ
```

**What happens behind the scenes:**

1. **First invoke**: State = `["Hi, my name is Nitesh", "Hello Nitesh"]`
2. **Workflow ends**: State is **erased** from memory
3. **Second invoke**: State = `["What is my name?"]` (previous messages gone!)
4. **LLM only sees**: "What is my name?" without context

### ğŸ’¡ The Solution: Persistence

We need to **save state** between workflow executions so it can be **retrieved** on the next invoke.

---

## Implementing Persistence

### ğŸ§  What is Persistence?

> **Persistence** means saving the state after workflow execution so it can be restored later.

**Two storage options:**

| Storage Type | Use Case | Persistence Duration |
|-------------|----------|---------------------|
| **In-Memory (RAM)** | Development, testing | Until program restarts |
| **Database** | Production | Permanent (survives restarts) |

### ğŸ”‘ Key Concepts

#### 1. **Checkpointers**
Objects that save and load state snapshots at each step of execution.

#### 2. **Threads**
Unique conversation sessions. Each user gets their own thread ID.

**Example:**
- Nitesh chatting â†’ Thread ID: "thread_1"
- Rahul chatting â†’ Thread ID: "thread_2"
- Amit chatting â†’ Thread ID: "thread_3"

Each thread maintains its own separate conversation history.

### ğŸ“ Implementation Steps

#### Step 1: Import MemorySaver

```python
from langgraph.checkpoint.memory import MemorySaver
```

**What is MemorySaver?**
- Built-in checkpointer that stores state in RAM
- Suitable for development and testing
- Data lost when program restarts

#### Step 2: Create Checkpointer

```python
# Create memory-based checkpointer
checkpointer = MemorySaver()
```

#### Step 3: Compile Graph with Checkpointer

```python
# Compile graph with persistence enabled
chatbot = graph.compile(checkpointer=checkpointer)
```

**What this does:**
- Tells LangGraph to save state after each execution
- Enables state retrieval before next execution

#### Step 4: Define Thread Configuration

```python
# Define thread ID for this conversation
thread_id = "thread_1"

# Create config with thread information
config = {
    "configurable": {
        "thread_id": thread_id
    }
}
```

**Why thread_id?**
- Identifies which conversation this message belongs to
- Allows multiple users to chat simultaneously
- Each thread maintains separate history

#### Step 5: Invoke with Config

```python
# Invoke chatbot with config to enable persistence
response = chatbot.invoke(
    {"messages": [HumanMessage(content=user_message)]},
    config=config  # Pass thread configuration
)
```

---

## Complete Working Code

### ğŸ¯ Full Chatbot with Memory

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI

# ============================================
# STEP 1: Define State
# ============================================
class ChatState(TypedDict):
    """
    Conversation state schema
    - messages: All conversation messages
    - add_messages: Reducer to append new messages
    """
    messages: Annotated[list[BaseMessage], add_messages]

# ============================================
# STEP 2: Define Chat Node
# ============================================
def chat_node(state: ChatState) -> dict:
    """
    Process user message and generate AI response
    
    Args:
        state: Current conversation state
        
    Returns:
        Dictionary with AI response message
    """
    # Initialize LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    
    # Get all messages from state
    messages = state["messages"]
    
    # Generate response
    response = llm.invoke(messages)
    
    # Return response in list format for reducer
    return {"messages": [response]}

# ============================================
# STEP 3: Build Graph
# ============================================
# Create checkpointer for persistence
checkpointer = MemorySaver()

# Initialize graph
graph = StateGraph(ChatState)

# Add chat node
graph.add_node("chat_node", chat_node)

# Define workflow edges
graph.add_edge("__start__", "chat_node")
graph.add_edge("chat_node", "__end__")

# Compile with checkpointer
chatbot = graph.compile(checkpointer=checkpointer)

# ============================================
# STEP 4: Chat Loop with Memory
# ============================================
# Define thread for this conversation
thread_id = "thread_1"

# Create config with thread ID
config = {
    "configurable": {
        "thread_id": thread_id
    }
}

# Main chat loop
while True:
    # Get user input
    user_message = input("You: ").strip()
    
    # Check for exit commands
    if user_message.lower() in ["exit", "quit", "bye"]:
        print("Goodbye!")
        break
    
    # Display user message
    print(f"User: {user_message}")
    
    # Invoke chatbot with persistence
    response = chatbot.invoke(
        {"messages": [HumanMessage(content=user_message)]},
        config=config  # Enable memory via thread
    )
    
    # Extract and display AI response
    ai_response = response["messages"][-1].content
    print(f"AI: {ai_response}\n")
```

### ğŸ§ª Testing the Chatbot

```python
# Conversation 1
You: Hi, my name is Nitesh
AI: Hello Nitesh! How can I assist you today?

# Conversation 2 - Memory works! âœ…
You: What is my name?
AI: Your name is Nitesh. How can I help you?

# Conversation 3 - Context maintained âœ…
You: Add 10 to 100
AI: Sure! 10 + 100 = 110

# Conversation 4 - Remembers previous result âœ…
You: Now multiply the result by 2
AI: Of course! 110 Ã— 2 = 220
```

### ğŸ” Inspecting State

```python
# Retrieve current state for thread
current_state = chatbot.get_state(config)

# View all messages in conversation
print(current_state["messages"])
```

**Output:**
```python
[
    HumanMessage(content="Hi, my name is Nitesh"),
    AIMessage(content="Hello Nitesh! How can I assist you today?"),
    HumanMessage(content="What is my name?"),
    AIMessage(content="Your name is Nitesh. How can I help you?"),
    HumanMessage(content="Add 10 to 100"),
    AIMessage(content="Sure! 10 + 100 = 110"),
    HumanMessage(content="Now multiply the result by 2"),
    AIMessage(content="Of course! 110 Ã— 2 = 220")
]
```

---

## How Persistence Works Behind the Scenes

### ğŸ”„