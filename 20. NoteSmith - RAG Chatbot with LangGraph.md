---
title: "NoteSmith - RAG Chatbot with LangGraph"
layout: default
nav_order: 18
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - RAG Chatbot with LangGraph"
last_modified_date: 2026-01-18
source_transcript: "020_RAG using LangGraph _ Agentic AI using LangGraph _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - RAG Chatbot with LangGraph

## Table of Contents

1. [Overview](#overview)
2. [What is RAG (Retrieval Augmented Generation)](#what-is-rag)
3. [Why RAG is Needed](#why-rag-is-needed)
4. [How RAG Works - Architecture](#how-rag-works)
5. [Building RAG in LangGraph](#building-rag-in-langgraph)
6. [Integration with Existing Chatbot](#integration-with-existing-chatbot)
7. [Quick Reference](#quick-reference)
8. [Summary Table](#summary-table)
9. [Key Takeaways](#key-takeaways)
10. [Edge Cases & Common Mistakes](#edge-cases-and-common-mistakes)
11. [Interview Questions](#interview-questions)

---

## Overview

### ðŸŽ¯ What This Covers

This lecture demonstrates how to build a **Multi-Utility Chatbot** using LangGraph that combines:
- Normal conversational abilities
- Tool integration (stock prices, calculations, web search)
- **RAG (Retrieval Augmented Generation)** for document-based Q&A
- MCP (Model Context Protocol) integration
- Streaming responses
- Conversation persistence

### ðŸ“‹ Prerequisites

- Basic understanding of LLMs and chatbots
- Familiarity with LangGraph fundamentals
- Python programming knowledge
- Understanding of tools/function calling in LLMs

### ðŸ’¡ Why This Matters

RAG is one of the most important patterns in production AI applications. It enables:
- Private document querying without retraining models
- Grounding LLM responses in factual data
- Building knowledge-based assistants
- Reducing hallucinations

**Industry Use Cases:**
- Customer support bots with company documentation
- Legal document analysis systems
- Medical record querying
- Code repository assistants
- Personal knowledge management tools

---

## What is RAG (Retrieval Augmented Generation)

> **RAG** is a technique that enhances LLM responses by retrieving relevant information from external knowledge sources and providing it as context during generation.

### ðŸ”‘ Core Principle: In-Context Learning

RAG works on the principle that LLMs can answer questions based on **additional context** provided in the prompt, even if that information wasn't in their training data.

**Simple Flow:**
```
User Question â†’ Retrieve Relevant Documents â†’ Add as Context â†’ LLM Generates Answer
```

**Without RAG:**
```python
# LLM only uses parametric knowledge (training data)
User: "What is machine learning?"
LLM: [Uses training knowledge] â†’ Response
```

**With RAG:**
```python
# LLM uses parametric knowledge + retrieved context
User: "What is machine learning?"
System: [Retrieves relevant pages from uploaded book]
LLM: [Uses training knowledge + book context] â†’ Response
```

---

## Why RAG is Needed

### 1ï¸âƒ£ **Outdated Knowledge** ðŸ“…

**Problem:** Every LLM has a **knowledge cutoff date** - the date when training was completed.

**Example:**
- GPT-5 trained until August 31, 2024
- Cannot answer questions about events after that date
- Knowledge becomes stale over time

**How RAG Solves This:**
- Retrieves current information from updated documents
- No need to retrain the model
- ChatGPT uses RAG with web search for recent queries

### 2ï¸âƒ£ **Privacy & Proprietary Data** ðŸ”’

**Problem:** LLMs don't have access to:
- Your company's internal documents
- Personal expense sheets
- Proprietary codebases
- Confidential reports

**Example Scenarios:**
```python
# These won't work without RAG
"Summarize our Q4 financial report"
"What were my expenses last month?"
"Explain the authentication module in our codebase"
```

**How RAG Solves This:**
- Upload private documents to a vector store
- LLM queries only your data
- Data never leaves your infrastructure
- No need to include sensitive info in training

### 3ï¸âƒ£ **Hallucination Reduction** ðŸŽ­

**Problem:** LLMs sometimes generate **false information with confidence**.

**Example:**
```python
User: "What are the top AI research papers of 2025?"
LLM: [Generates 10 paper links]
Reality: 4 links return 404 errors (papers don't exist)
```

**How RAG Solves This:**
- **Grounds** responses in actual documents
- LLM instructed to answer only from provided context
- Reduces fabricated information
- Enables citation/source tracking

---

## How RAG Works - Architecture

### ðŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG WORKFLOW                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INDEXING PHASE (One-time setup):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document   â”‚â”€â”€â”€â–¶â”‚    Split     â”‚â”€â”€â”€â–¶â”‚  Generate    â”‚
â”‚  (100 pages) â”‚    â”‚  into Chunks â”‚    â”‚  Embeddings  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚Vector Store  â”‚
                                        â”‚  (FAISS/     â”‚
                                        â”‚   Chroma)    â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERY PHASE (Every user question):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query:  â”‚â”€â”€â”€â–¶â”‚  Retriever   â”‚â”€â”€â”€â–¶â”‚ Vector Store â”‚
â”‚"What is ML?" â”‚    â”‚  (Converts   â”‚    â”‚   Search     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ to embedding)â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                                               â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚ Top-K Most   â”‚
                                        â”‚  Similar     â”‚
                                        â”‚  Chunks      â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROMPT TO LLM                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Question: What is machine learning?           â”‚ â”‚
â”‚  â”‚                                                â”‚ â”‚
â”‚  â”‚ Context:                                       â”‚ â”‚
â”‚  â”‚ [Page 1 content about ML...]                  â”‚ â”‚
â”‚  â”‚ [Page 5 content about ML...]                  â”‚ â”‚
â”‚  â”‚ [Page 99 content about ML...]                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     LLM      â”‚
            â”‚  Generates   â”‚
            â”‚   Answer     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ”§ Step-by-Step Process

#### **Phase 1: Indexing (Setup)**

**Step 1: Load Document**
```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("intro_to_ml.pdf")
documents = loader.load()
# Result: List of Document objects (one per page)
```

**Step 2: Split into Chunks**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Each chunk ~1000 characters
    chunk_overlap=200     # 200 char overlap between chunks
)
chunks = text_splitter.split_documents(documents)
# Result: 100-page book â†’ ~300 chunks
```

**Why Split?**
- LLM context windows are limited (e.g., 128K tokens)
- A 1000-page book won't fit entirely
- Smaller chunks = better semantic matching
- Overlap preserves context between chunks

**Step 3: Generate Embeddings**
```python
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
# This model converts text â†’ vector of numbers
```

**What are Embeddings?**
- Numerical representation of text (e.g., [0.23, -0.45, 0.67, ...])
- Captures **semantic meaning**
- Similar text â†’ similar vectors
- Typical size: 384 to 1536 dimensions

**Step 4: Store in Vector Database**
```python
from langchain_community.vectorstores import FAISS

vector_store = FAISS.from_documents(
    documents=chunks,
    embedding=embedding_model
)
# Stores: [embedding_vector, original_text, metadata]
```

**Popular Vector Stores:**
- **FAISS** (Facebook AI Similarity Search) - Fast, local
- **Chroma** - Open-source, persistent
- **Pinecone** - Cloud-based, scalable
- **Weaviate** - Production-grade

#### **Phase 2: Querying (Runtime)**

**Step 1: Create Retriever**
```python
retriever = vector_store.as_retriever(
    search_type="similarity",  # Semantic similarity search
    search_kwargs={"k": 4}     # Return top 4 matches
)
```

**Step 2: User Asks Question**
```python
query = "What is machine learning?"
```

**Step 3: Retrieve Relevant Chunks**
```python
# Behind the scenes:
# 1. Convert query to embedding
query_embedding = embedding_model.embed_query(query)

# 2. Find similar vectors in database
results = retriever.invoke(query)
# Returns: 4 most similar chunks (pages 1, 5, 99, 45)
```

**Step 4: Construct Prompt**
```python
context = "\n\n".join([doc.page_content for doc in results])

prompt = f"""
Question: {query}

Context:
{context}

Answer the question based only on the context provided.
"""
```

**Step 5: LLM Generates Answer**
```python
response = llm.invoke(prompt)
# LLM uses: parametric knowledge + retrieved context
```

---

## Building RAG in LangGraph

### ðŸ—ï¸ Architecture Pattern: RAG as a Tool

In LangGraph, we implement RAG as a **tool** that the agent can call when needed.

**Flow:**
```
User Query â†’ Chat Node â†’ Decides if RAG needed
                â†“ Yes
            RAG Tool â†’ Retrieves context â†’ Returns to Chat Node
                â†“
            Chat Node â†’ Generates final answer
```

### ðŸ“ Complete Implementation

#### **Step 1: Setup and Imports**

```python
# Install required packages
!pip install langchain langchain-openai langchain-community
!pip install faiss-cpu pypdf langgraph python-dotenv

# Import libraries
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.tools import tool
from langgraph.graph import StateGraph, MessagesState
from langgraph.prebuilt import ToolNode
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
os.environ["OPENAI_API_KEY"] = "your-api-key"
```

#### **Step 2: Initialize LLM**

```python
# Create LLM instance
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0  # Deterministic responses
)
```

**Why GPT-4o-mini?**
- Cost-effective for development
- Fast response times
- Good balance of capability and speed
- Can be replaced with GPT-4 for production

#### **Step 3: Document Ingestion Pipeline**

```python
# Load PDF document
loader = PyPDFLoader("intro_to_ml.pdf")
documents = loader.load()
print(f"Loaded {len(documents)} pages")

# Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # ~1000 characters per chunk
    chunk_overlap=200,    # Overlap to preserve context
    length_function=len,  # Use character count
    separators=["\n\n", "\n", " ", ""]  # Split priority
)
chunks = text_splitter.split_documents(documents)
print(f"Created {len(chunks)} chunks")

# Create embedding model
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# Generate embeddings and store in vector database
vector_store = FAISS.from_documents(
    documents=chunks,
    embedding=embedding_model
)
print("Vector store created successfully")

# Create retriever
retriever = vector_store.as_retriever(
    search_type="similarity",  # Use semantic similarity
    search_kwargs={"k": 4}     # Return top 4 results
)
```

**Line-by-Line Breakdown:**

1. **PyPDFLoader**: Extracts text from PDF, creates Document objects
2. **RecursiveCharacterTextSplitter**: 
   - Tries to split on `\n\n` first (paragraphs)
   - Falls back to `\n` (lines), then spaces
   - Ensures chunks don't exceed 1000 chars
   - 200-char overlap prevents context loss
3. **OpenAIEmbeddings**: Converts text to 1536-dimensional vectors
4. **FAISS.from_documents**: 
   - Generates embeddings for all chunks
   - Builds similarity search index
   - Stores in memory (can be persisted to disk)
5. **as_retriever**: Creates query interface with similarity search

#### **Step 4: Define RAG Tool**

```python
@tool
def rag_tool(query: str) -> dict:
    """
    Retrieves relevant information from uploaded PDF documents.
    Use this tool when the user asks questions about uploaded documents.
    
    Args:
        query: The user's question about the document
        
    Returns:
        Dictionary containing query, context, and metadata
    """
    # Retrieve relevant documents
    results = retriever.invoke(query)
    
    # Extract page content from each result
    context = [doc.page_content for doc in results]
    
    # Extract metadata (page numbers, source, etc.)
    metadata = [doc.metadata for doc in results]
    
    # Return structured response
    return {
        "query": query,
        "context": context,
        "metadata": metadata
    }
```

**Tool Breakdown:**

- **@tool decorator**: Converts function to LangChain tool
- **Docstring**: LLM reads this to understand when to use the tool
- **retriever.invoke(query)**: 
  - Converts query to embedding
  - Searches vector store
  - Returns top-k similar chunks
- **Return format**: Structured dict for easy parsing

**Example Usage:**
```python
# Manual test
result = rag_tool.invoke("What is a decision tree?")
print(result["context"][0])  # First retrieved chunk
```

#### **Step 5: Create LangGraph Workflow**

```python
# Bind tools to LLM
tools = [rag_tool]
llm_with_tools = llm.bind_tools(tools)

# Define chat node
def chat_node(state: MessagesState):
    """
    Main chat node that decides whether to use tools or respond directly.
    """
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

# Create tool node
tool_node = ToolNode(tools)

# Build graph
workflow = StateGraph(MessagesState)

# Add nodes