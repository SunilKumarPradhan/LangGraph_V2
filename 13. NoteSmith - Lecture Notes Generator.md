---
title: "NoteSmith - Lecture Notes Generator"
layout: default
nav_order: 12
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes Generator"
last_modified_date: 2026-01-18
source_transcript: "013_Streaming in LangGraph _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes Generator
# Streaming in LangGraph Chatbots üöÄ

---

## Table of Contents

1. [Overview](#overview)
2. [What is Streaming?](#what-is-streaming)
3. [Why Streaming Matters](#why-streaming-matters)
4. [Technical Foundation](#technical-foundation)
5. [Implementation in LangGraph](#implementation-in-langgraph)
6. [Frontend Integration with Streamlit](#frontend-integration-with-streamlit)
7. [Quick Reference](#quick-reference)
8. [Summary Table](#summary-table)
9. [Key Takeaways](#key-takeaways)
10. [Edge Cases & Common Mistakes](#edge-cases--common-mistakes)
11. [Interview Questions](#interview-questions)

---

## Overview

### üìö What This Covers

This lecture continues the **Agentic AI using LangGraph** series, focusing on implementing **streaming functionality** in chatbots. We'll transform a basic chatbot that displays responses all at once into one that shows responses token-by-token with a typewriter effect.

### üéØ Prerequisites

- Basic understanding of LangGraph
- Python fundamentals
- Knowledge of chatbot architecture
- Familiarity with Streamlit (for UI)
- Understanding of Python generators (helpful but will be covered)

### üí° Why It Matters

Streaming is **critical** for production-grade LLM applications because:
- It dramatically improves user experience
- Reduces perceived wait time
- Makes applications feel more human-like
- Saves tokens (and money) when users stop responses early
- Essential for voice-based AI assistants

---

## What is Streaming? üåä

### Basic Definition

> **Streaming** in LLMs means the model starts sending tokens as soon as they are generated, instead of waiting for the entire response to be ready before returning it.

### The Two Approaches

| **Without Streaming** | **With Streaming** |
|----------------------|-------------------|
| User sends query ‚Üí LLM generates complete response ‚Üí Entire response appears at once | User sends query ‚Üí LLM generates tokens ‚Üí Tokens appear one-by-one as generated |
| 5-10 second blank screen | Immediate feedback starts |
| Abrupt, jarring experience | Smooth, typewriter effect |
| All-or-nothing display | Progressive rendering |

### Real-World Analogy

Think of it like:
- **Without Streaming**: Ordering food and the waiter brings everything at once after 30 minutes
- **With Streaming**: Ordering a multi-course meal where dishes arrive as they're prepared

---

## Why Streaming Matters üéØ

### 1. ‚ö° Faster Response Time (Perceived)

**Problem Without Streaming:**
```
User: "Write a 500-word blog on cricket"
[10 seconds of blank screen]
User: "Is the app frozen? Should I refresh?"
[Suddenly entire blog appears]
```

**Solution With Streaming:**
```
User: "Write a 500-word blog on cricket"
[Immediately starts typing]
"Cricket is one of the most popular..."
[User sees progress, stays engaged]
```

**Why It Matters:**
- Non-technical users may think the app crashed
- High drop-off rates during wait times
- Poor user retention

### 2. ü§ù Mimics Human-Like Conversation

**Key Benefits:**
- **Builds Trust**: Feels like talking to a real person
- **Feels Alive**: Dynamic, responsive interaction
- **Keeps Users Engaged**: Every moment has new content

**Example:**
ChatGPT's success is partly due to its streaming interface making conversations feel natural.

### 3. üéôÔ∏è Critical for Multimodal UI

**Voice Assistant Scenario:**

```python
# Without Streaming
User: "Give me the recipe to cook pasta"
[15 seconds of silence]
Alexa: [Suddenly starts speaking entire recipe]
# Awkward, feels broken

# With Streaming
User: "Give me the recipe to cook pasta"
[Immediate response]
Alexa: "First, boil water in a large pot..."
# Natural conversation flow
```

### 4. üìù Better UX for Long Outputs (Code, Essays)

**Code Generation Example:**

Without streaming:
```
[Wait... wait... wait...]
[BOOM - 200 lines of code appear]
[User overwhelmed, can't follow structure]
```

With streaming:
```python
# User sees code build line-by-line
def calculate_sum(numbers):
    """Calculate sum of numbers"""
    total = 0
    for num in numbers:  # User can follow logic as it appears
        total += num
    return total
```

### 5. üí∞ Cost Savings Through Early Stopping

**Token Economics:**

```python
# User asks for essay
# After 50 tokens, user realizes it's not what they wanted
# With streaming: User clicks "Stop" ‚Üí Only charged for 50 tokens
# Without streaming: Full 500 tokens generated ‚Üí Charged for all

# Savings = 450 tokens √ó $0.002 per 1K tokens = Significant at scale
```

### 6. üìä Real-Time Progress Updates (AI Agents)

**Agent Task Example:**

```
User: "Book a movie ticket for me"

Without Streaming:
[60 seconds of nothing]
"Your ticket is booked!"

With Streaming:
"Opening BookMyShow..." ‚úì
"Searching for available shows..." ‚úì
"Found 5 shows for Avengers..." ‚úì
"Selecting 7 PM show..." ‚úì
"Choosing seats A1, A2..." ‚úì
"Processing payment..." ‚úì
"Ticket booked successfully!" ‚úì
```

---

## Technical Foundation üîß

### Understanding Python Generators

Before implementing streaming, you must understand **generators**.

#### What is a Generator?

> A **generator** in Python is a special type of iterator that allows you to generate values on-the-fly, one at a time, using the `yield` keyword instead of `return`.

#### Regular Function vs Generator

```python
# ‚ùå Regular Function - Returns all at once
def get_numbers():
    return [1, 2, 3, 4, 5]

result = get_numbers()
print(result)  # [1, 2, 3, 4, 5] - All in memory at once

# ‚úÖ Generator Function - Yields one at a time
def get_numbers_generator():
    yield 1
    yield 2
    yield 3
    yield 4
    yield 5

result = get_numbers_generator()
print(result)  # <generator object> - Not the values yet!

# Access values one by one
for num in result:
    print(num)  # Prints 1, then 2, then 3, etc.
```

#### Why Generators for Streaming?

```python
# Memory Efficient Example
def stream_large_file():
    """
    Instead of loading entire file in memory,
    yield one line at a time
    """
    with open('huge_file.txt') as f:
        for line in f:
            yield line.strip()

# Process 1GB file without loading all in RAM
for line in stream_large_file():
    process(line)  # Only one line in memory at a time
```

#### Complete Generator Example

```python
def countdown(n):
    """Generator that counts down from n to 1"""
    while n > 0:
        yield n  # Pause here, return n, wait for next() call
        n -= 1

# Using the generator
counter = countdown(5)

print(next(counter))  # 5
print(next(counter))  # 4
print(next(counter))  # 3

# Or loop through all
for num in countdown(5):
    print(num)  # 5, 4, 3, 2, 1
```

---

## Implementation in LangGraph üíª

### Step 1: Understanding the Change

The **only** change needed in backend code:

```python
# ‚ùå OLD WAY - Without Streaming
response = chatbot.invoke(initial_state)
print(response)  # Entire response at once

# ‚úÖ NEW WAY - With Streaming
stream = chatbot.stream(initial_state, config, stream_mode="messages")
# Returns a generator object
```

### Step 2: LangGraph Stream Modes

LangGraph provides multiple streaming modes:

| Mode | Use Case | Returns |
|------|----------|---------|
| `"messages"` | Stream LLM responses token-by-token | Message chunks |
| `"updates"` | Stream state updates | State changes |
| `"values"` | Stream complete state values | Full state objects |
| `"custom"` | Custom streaming logic | User-defined |

**For chatbots displaying LLM text, use `"messages"`**

### Step 3: Backend Implementation

```python
# backend.py
from langgraph.graph import StateGraph
from langchain_core.messages import HumanMessage

# Assume chatbot graph is already built
# chatbot = StateGraph(...)

# Configuration
initial_state = {
    "messages": [HumanMessage(content="What is the recipe to make pasta?")]
}

config = {
    "configurable": {"thread_id": "1"}
}

# üî• THE KEY CHANGE - Use .stream() instead of .invoke()
stream = chatbot.stream(
    initial_state,
    config=config,
    stream_mode="messages"  # Critical for token-by-token streaming
)

# Verify it's a generator
print(type(stream))  # <class 'generator'>

# Loop through generator to print tokens
for message_chunk, metadata in stream:
    # message_chunk contains the token
    # metadata contains additional info
    if hasattr(message_chunk, 'content'):
        print(message_chunk.content, end=" ")  # Print without newline
```

#### Line-by-Line Breakdown

```python
# Line 1: Call stream method
stream = chatbot.stream(
    initial_state,           # Your input messages
    config=config,           # Thread ID for conversation memory
    stream_mode="messages"   # Mode for token streaming
)

# Line 2: Loop through generator
for message_chunk, metadata in stream:
    # message_chunk: Contains the actual token/text
    # metadata: Contains info about the chunk (node name, etc.)
    
    # Line 3: Check if content exists
    if hasattr(message_chunk, 'content'):
        # Some chunks might be empty or control messages
        
        # Line 4: Print the content
        print(message_chunk.content, end=" ")
        # end=" " prevents newline, creates continuous text
```

### Step 4: Complete Working Example

```python
# complete_backend_streaming.py

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from typing import TypedDict, Annotated, Sequence
import operator

# Define State
class State(TypedDict):
    messages: Annotated[Sequence, operator.add]

# Create LLM
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# Define chatbot node
def chatbot_node(state: State):
    """Process messages and return LLM response"""
    return {"messages": [llm.invoke(state["messages"])]}

# Build graph
workflow = StateGraph(State)
workflow.add_node("chatbot", chatbot_node)
workflow.set_entry_point("chatbot")
workflow.add_edge("chatbot", END)

# Compile with memory
memory = MemorySaver()
chatbot = workflow.compile(checkpointer=memory)

# ========== STREAMING IMPLEMENTATION ==========

def stream_chatbot_response(user_message: str, thread_id: str = "1"):
    """
    Stream chatbot response token by token
    
    Args:
        user_message: User's input text
        thread_id: Conversation thread identifier
    """
    # Prepare initial state
    initial_state = {
        "messages": [HumanMessage(content=user_message)]
    }
    
    # Configuration
    config = {
        "configurable": {"thread_id": thread_id}
    }
    
    # Stream the response
    print(f"User: {user_message}\n")
    print("Assistant: ", end="")
    
    for message_chunk, metadata in chatbot.stream(
        initial_state,
        config=config,
        stream_mode="messages"
    ):
        # Extract and print content
        if hasattr(message_chunk, 'content') and message_chunk.content:
            print(message_chunk.content, end="", flush=True)
    
    print("\n")  # New line after complete response

# Test the streaming
if __name__ == "__main__":
    stream_chatbot_response("What is the recipe to make pasta?")
    stream_chatbot_response("Make it shorter", thread_id="1")  # Same thread
```

---

## Frontend Integration with Streamlit üé®

### Understanding Streamlit's Chat Elements

Streamlit provides specialized chat UI components:

| Component | Purpose | Example |
|-----------|---------|---------|
| `st.chat_input()` | User input box | `user_input = st.chat_input("Type here")` |
| `st.chat_message()` | Message container | `with st.chat_message("user"):` |
| `st.write_stream()` | **Stream with typewriter effect** | `st.write_stream(generator)` |
| `st.status()` | Show progress updates | For agent task updates |

### The Magic Function: `st.write_stream()`

```python
# st.write_stream() documentation
st.write_stream(stream)
"""
Writes generators and streams to the app with a typewriter effect.

Parameters:
    stream: A generator or iterable that yields strings

Returns:
    The complete text after streaming finishes
"""
```

### Step-by-Step Frontend Implementation

#### Before Streaming (Old Code)

```python
# frontend.py - OLD VERSION
import streamlit as st
from backend import chatbot

st.title("My Chatbot")

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# User input
if user_input := st.chat_input("Type your message"):
    # Add user message to session
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })
    
    # Display user message
    with st.chat_message("user"):
        st.write(user_input)
    
    # Get AI response - ALL AT ONCE ‚ùå
    with st.chat_message("assistant"):
        initial_state = {"messages": [{"role": "user", "content": user_input}]}
        config = {"configurable": {"thread_id": "1"}}
        
        response = chatbot.invoke(initial_state, config=config)
        ai_message = response["messages"][-1].content
        
        st.write(ai_message)  # Appears all at once
    
    # Save to session
    st.session_state.messages.append({
        "role": "assistant",
        "content": ai_message
    })
```

#### After Streaming (New Code)

```python
# frontend_streaming.py - NEW VERSION
import streamlit as st
from backend import chatbot
from langchain_core.messages import HumanMessage

st.title("My Chatbot with Streaming ‚ú®")

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# User input
if user_input := st.chat_input("Type your message"):
    # Add user message to session
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })
    
    # Display user message
    with st.chat_message("user"):
        st.write(user_input)
    
    # üî• STREAMING IMPLEMENTATION
    with st.chat_message("assistant"):
        # Prepare streaming
        initial_state = {
            "messages": [HumanMessage(content=user_input