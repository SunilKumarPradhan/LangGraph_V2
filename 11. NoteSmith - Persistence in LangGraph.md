---
title: "NoteSmith - Persistence in LangGraph"
layout: default
nav_order: 10
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Persistence in LangGraph"
last_modified_date: 2026-01-18
source_transcript: "011_Persistence in LangGraph _ Time Travel in LangGraph _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - Persistence in LangGraph

## Table of Contents

1. [Overview](#overview)
2. [What is Persistence?](#what-is-persistence)
3. [Core Concepts Review](#core-concepts-review)
4. [Understanding Persistence in Depth](#understanding-persistence-in-depth)
5. [Checkpointers - The Implementation Mechanism](#checkpointers---the-implementation-mechanism)
6. [Thread IDs - Managing Multiple Executions](#thread-ids---managing-multiple-executions)
7. [Practical Implementation](#practical-implementation)
8. [Benefits of Persistence](#benefits-of-persistence)
9. [Quick Reference](#quick-reference)
10. [Summary Table](#summary-table)
11. [Key Takeaways](#key-takeaways)
12. [Common Mistakes & Edge Cases](#common-mistakes--edge-cases)
13. [Interview Questions](#interview-questions)

---

## Overview

### What This Covers
This lecture explores **Persistence in LangGraph** - a foundational concept that enables saving and restoring workflow state over time. You'll learn how persistence powers critical features like fault tolerance, chat memory, and human-in-the-loop workflows.

### Prerequisites
- Basic understanding of LangGraph graphs and nodes
- Familiarity with the concept of State in LangGraph
- Knowledge of workflow execution using `invoke()`

### Why It Matters
Persistence is not just a feature‚Äîit's a **foundational pillar** upon which many advanced LangGraph capabilities are built. Without persistence:
- ‚ùå Chatbots can't remember past conversations
- ‚ùå Workflows can't recover from crashes
- ‚ùå Human approval steps become impossible
- ‚ùå Debugging complex workflows is extremely difficult

---

## What is Persistence?

> **Persistence in LangGraph refers to the ability to save and restore the state of a workflow over time.**

### The Default Behavior (Without Persistence)

When you execute a LangGraph workflow normally:

1. **Input** ‚Üí Workflow starts
2. **Execution** ‚Üí Nodes process sequentially/parallel
3. **State Changes** ‚Üí State values update at each node
4. **Completion** ‚Üí Workflow ends
5. **‚ùå State Lost** ‚Üí All state values are erased from RAM

```
Start ‚Üí Node1 ‚Üí Node2 ‚Üí End
         ‚Üì       ‚Üì       ‚Üì
      State‚ÇÅ  State‚ÇÇ  State‚ÇÉ  ‚Üí üí• ALL DELETED after execution
```

### With Persistence Enabled

```
Start ‚Üí Node1 ‚Üí Node2 ‚Üí End
         ‚Üì       ‚Üì       ‚Üì
      State‚ÇÅ  State‚ÇÇ  State‚ÇÉ  ‚Üí ‚úÖ ALL SAVED to database
         ‚Üì       ‚Üì       ‚Üì
      [DB]    [DB]    [DB]    ‚Üí Can retrieve anytime later
```

---

## Core Concepts Review

Before diving deeper, let's revisit two fundamental LangGraph principles:

### 1Ô∏è‚É£ Concept of Graph

**High-level goal** ‚Üí Decomposed into **set of tasks** ‚Üí Represented as **graph**

```
Graph Structure:
- Nodes = Individual tasks
- Edges = Execution order (which task runs after which)
```

**Example:**
```
Task1 ‚Üí Task2 ‚Üí Task3
  ‚Üì       ‚Üì       ‚Üì
Node1   Node2   Node3
```

### 2Ô∏è‚É£ Concept of State

**State = Dictionary storing important data needed during workflow execution**

Key characteristics:
- ‚úÖ **Accessible** from any node
- ‚úÖ **Readable** by all nodes
- ‚úÖ **Writable** by all nodes
- üì¶ Stores critical workflow data (messages, variables, results)

**Example - Chatbot State:**
```python
{
    "messages": [...],  # Conversation history
    "user_id": "123",
    "context": {...}
}
```

---

## Understanding Persistence in Depth

### üîë Critical Feature: Saves ALL State Values

Persistence doesn't just save the **final state** - it saves **every intermediate state** at each checkpoint.

#### Example Workflow

```
Start ‚Üí Node1 ‚Üí Node2 ‚Üí End
  ‚Üì       ‚Üì       ‚Üì       ‚Üì
name=A  name=B  name=C  name=C
```

**What gets saved:**
1. ‚úÖ Checkpoint 1: `name = "A"` (at Start)
2. ‚úÖ Checkpoint 2: `name = "B"` (after Node1)
3. ‚úÖ Checkpoint 3: `name = "C"` (after Node2)
4. ‚úÖ Checkpoint 4: `name = "C"` (at End)

### Why Save Intermediate States?

#### Use Case 1: Fault Tolerance

```
Start ‚Üí Node1 ‚Üí Node2 üí• CRASH ‚Üí Node3 ‚Üí End
         ‚úÖ      ‚úÖ (saved)
```

**Recovery:**
```python
# Resume from where it crashed
workflow.invoke(None, config={"thread_id": "1"})
# Starts from Node2, NOT from Start!
```

#### Use Case 2: Resume Chat Conversations

```
Day 1: User asks about Python ‚Üí Conversation saved
Day 3: User returns ‚Üí Load saved conversation ‚Üí Continue from where left off
```

---

## Checkpointers - The Implementation Mechanism

### What is a Checkpointer?

A **checkpointer** is the component that implements persistence by:
1. Dividing workflow execution into **checkpoints**
2. Saving state values at each checkpoint
3. Storing data in a database (or memory)

### How Checkpoints are Created

**Rule:** Every **superstep** becomes a checkpoint.

#### What is a Superstep?

A superstep includes all operations that happen in parallel at the same "level" of execution.

**Example Graph:**

```
        Start
          ‚Üì
        Node1  ‚Üê Superstep 1
       /  |  \
   Node2 Node3 Node4  ‚Üê Superstep 2 (all parallel)
       \  |  /
         End  ‚Üê Superstep 3
```

**Checkpoints created:**
- ‚úÖ Checkpoint 1: Before Node1
- ‚úÖ Checkpoint 2: After Node1, before Node2/3/4
- ‚úÖ Checkpoint 3: After Node2/3/4, before End
- ‚úÖ Checkpoint 4: At End

### Types of Checkpointers

| Checkpointer | Storage | Use Case |
|--------------|---------|----------|
| `MemorySaver` | RAM | Demos, testing, learning |
| `PostgresSaver` | PostgreSQL DB | Production chatbots |
| `RedisSaver` | Redis | High-performance production |
| `SqliteSaver` | SQLite file | Local development |

‚ö†Ô∏è **Important:** `MemorySaver` loses data when program stops - use only for demos!

---

## Thread IDs - Managing Multiple Executions

### The Problem

```
Execution 1: Topic="Pizza" ‚Üí Joke‚ÇÅ ‚Üí Explanation‚ÇÅ ‚Üí Save to DB
Execution 2: Topic="Pasta" ‚Üí Joke‚ÇÇ ‚Üí Explanation‚ÇÇ ‚Üí Save to DB

Question: How do we differentiate between these two executions in the database?
```

### The Solution: Thread IDs

Each workflow execution gets a **unique thread ID**. All state values for that execution are stored against this thread ID.

```python
# Execution 1
workflow.invoke(
    {"topic": "Pizza"},
    config={"configurable": {"thread_id": "1"}}
)
# All states saved with thread_id=1

# Execution 2
workflow.invoke(
    {"topic": "Pasta"},
    config={"configurable": {"thread_id": "2"}}
)
# All states saved with thread_id=2
```

### Retrieving Specific Execution Data

```python
# Get Pizza execution final state
workflow.get_state(config={"configurable": {"thread_id": "1"}})

# Get Pasta execution final state
workflow.get_state(config={"configurable": {"thread_id": "2"}})

# Get Pizza execution history (all intermediate states)
workflow.get_state_history(config={"configurable": {"thread_id": "1"}})
```

### Real-World Example: Chatbot

```python
# User starts new conversation
thread_id = "user123_conv1"
workflow.invoke({"message": "Hello"}, config={"thread_id": thread_id})

# 3 days later, user returns
# Load same thread_id to resume conversation
workflow.invoke({"message": "Continue"}, config={"thread_id": thread_id})
```

---

## Practical Implementation

### Example: Joke Generator with Persistence

**Workflow:**
```
Start ‚Üí Generate Joke ‚Üí Generate Explanation ‚Üí End
```

### Step 1: Import Checkpointer

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from typing import TypedDict

# Initialize LLM
llm = ChatOpenAI(model="gpt-4")
```

### Step 2: Define State

```python
class JokeState(TypedDict):
    topic: str           # Input: topic for joke
    joke: str           # Generated joke text
    explanation: str    # Explanation of the joke
```

**Explanation:**
- `topic`: User provides this (e.g., "Pizza")
- `joke`: Generated by first node
- `explanation`: Generated by second node

### Step 3: Create Node Functions

```python
def generate_joke(state: JokeState) -> JokeState:
    """Generate a joke on the given topic"""
    topic = state["topic"]
    
    # Create prompt
    prompt = f"Generate a joke on the topic: {topic}"
    
    # Get response from LLM
    response = llm.invoke(prompt)
    
    # Update state with joke
    return {"joke": response.content}

def generate_explanation(state: JokeState) -> JokeState:
    """Generate explanation for the joke"""
    joke = state["joke"]
    
    # Create prompt
    prompt = f"Write an explanation for the joke: {joke}"
    
    # Get response from LLM
    response = llm.invoke(prompt)
    
    # Update state with explanation
    return {"explanation": response.content}
```

**Line-by-line breakdown:**

**`generate_joke` function:**
- Line 1: Function takes current state as input
- Line 3: Extract topic from state dictionary
- Line 6: Create prompt asking LLM to generate joke
- Line 9: Send prompt to LLM and get response
- Line 12: Return dictionary with joke (merges into state)

**`generate_explanation` function:**
- Line 1: Function takes current state as input
- Line 3: Extract previously generated joke from state
- Line 6: Create prompt asking LLM to explain the joke
- Line 9: Send prompt to LLM and get response
- Line 12: Return dictionary with explanation (merges into state)

### Step 4: Build Graph with Persistence

```python
# Create checkpointer object
checkpointer = MemorySaver()

# Build graph
workflow = StateGraph(JokeState)

# Add nodes
workflow.add_node("generate_joke", generate_joke)
workflow.add_node("generate_explanation", generate_explanation)

# Add edges (execution flow)
workflow.add_edge("__start__", "generate_joke")
workflow.add_edge("generate_joke", "generate_explanation")
workflow.add_edge("generate_explanation", "__end__")

# Compile with checkpointer - THIS ENABLES PERSISTENCE
app = workflow.compile(checkpointer=checkpointer)
```

**Critical line:**
```python
app = workflow.compile(checkpointer=checkpointer)
```
This single line enables persistence! Without it, state values are lost after execution.

### Step 5: Execute Workflow

```python
# Configuration with thread ID
config = {
    "configurable": {
        "thread_id": "1"  # Unique identifier for this execution
    }
}

# Execute workflow
result = app.invoke(
    {"topic": "Pizza"},  # Initial state
    config=config        # Thread ID for persistence
)

print(result)
```

**Output:**
```python
{
    'topic': 'Pizza',
    'joke': 'Why did the pizza go to the doctor? Because it was feeling a little cheesy!',
    'explanation': 'This joke is a play on words...'
}
```

### Step 6: Retrieve Final State

```python
# Get final state for thread_id="1"
final_state = app.get_state(config={"configurable": {"thread_id": "1"}})

print(final_state.values)
```

**Output:**
```python
{
    'topic': 'Pizza',
    'joke': 'Why did the pizza go to the doctor?...',
    'explanation': 'This joke is a play on words...'
}
```

### Step 7: Retrieve State History (All Checkpoints)

```python
# Get all intermediate states
history = app.get_state_history(config={"configurable": {"thread_id": "1"}})

for state in history:
    print(f"Checkpoint: {state.config['configurable']['checkpoint_id']}")
    print(f"Values: {state.values}")
    print(f"Next node: {state.next}")
    print("---")
```

**Output:**
```
Checkpoint: checkpoint_4
Values: {'topic': 'Pizza', 'joke': '...', 'explanation': '...'}
Next node: ()
---
Checkpoint: checkpoint_3
Values: {'topic': 'Pizza', 'joke': '...'}
Next node: ('generate_explanation',)
---
Checkpoint: checkpoint_2
Values: {'topic': 'Pizza'}
Next node: ('generate_joke',)
---
Checkpoint: checkpoint_1
Values: {}
Next node: ('__start__',)
```

**Understanding the output:**
- **Checkpoint 1**: Empty state, about to start
- **Checkpoint 2**: Topic received, about to generate joke
- **Checkpoint 3**: Joke generated, about to generate explanation
- **Checkpoint 4**: Everything complete, no next node

### Step 8: Multiple Executions with Different Thread IDs

```python
# Second execution with different topic
config2 = {"configurable": {"thread_id": "2"}}

result2 = app.invoke(
    {"topic": "Pasta"},
    config=config2
)

# Retrieve Pizza execution
pizza_state = app.get_state(config={"configurable": {"thread_id": "1"}})

# Retrieve Pasta execution
pasta_state = app.get_state(config={"configurable": {"thread_id": "2"}})

print("Pizza joke:", pizza_state.values["joke"])
print("Pasta joke:", pasta_state.values["joke"])
```

Both executions are stored independently in the database!

---

## Benefits of Persistence

### 1Ô∏è‚É£ Short-Term Memory (Chatbots)

**Problem:** Users want to resume past conversations.

**Solution:** Store all messages with persistence.

```python
# Day 1: User starts conversation
config = {"configurable": {"thread_id": "user123_session1"}}
chatbot.invoke({"message": "Explain Python"}, config=config)

# Day 3: User returns
# Load same thread_id to show past messages
past_messages = chatbot.get_state(config)
# User sees entire conversation history!
```

**How it works:**
1. Each message exchange is saved at checkpoints
2. Thread ID links all messages of one conversation
3. Loading thread ID retrieves entire chat history
4. User can continue from where they left off

### 2Ô∏è‚É£ Fault Tolerance

**Problem:** Workflow crashes mid-execution due to:
- Server downtime
- API failures
- Network issues
- Resource exhaustion

**Solution:** Resume from exact crash point.

#### Demonstration Code

```python
import time
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph
from typing import TypedDict

# Define state
class WorkflowState(TypedDict):
    input: str
    step1: str
    step2: str
    step3: str

# Node functions
def step1(state):
    print("Step 1 executed")
    return {"step1": "done"}

def step2(state):
    print("Step 2 executing...")
    time