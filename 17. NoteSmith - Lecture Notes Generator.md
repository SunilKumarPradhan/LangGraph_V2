---
title: "NoteSmith - Lecture Notes Generator"
layout: default
nav_order: 15
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes Generator"
last_modified_date: 2026-01-18
source_transcript: "017_Observability in LangGraph _ LangSmith Integration with LangGraph"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes Generator

# Observability in Agentic AI with LangSmith

## Table of Contents

1. [Overview](#overview)
2. [Journey Recap: Agentic AI Playlist](#journey-recap)
3. [What is Observability?](#what-is-observability)
4. [Why Observability Matters](#why-observability-matters)
5. [Introduction to LangSmith](#introduction-to-langsmith)
6. [Setting Up LangSmith](#setting-up-langsmith)
7. [Understanding Traces](#understanding-traces)
8. [Implementing Thread-Based Organization](#implementing-thread-based-organization)
9. [Code Implementation Deep Dive](#code-implementation-deep-dive)
10. [LangSmith Dashboard Features](#langsmith-dashboard-features)
11. [Quick Reference](#quick-reference)
12. [Summary Table](#summary-table)
13. [Key Takeaways](#key-takeaways)
14. [Edge Cases & Common Mistakes](#edge-cases--common-mistakes)
15. [Interview Questions](#interview-questions)

---

## Overview

### ğŸ“š What This Covers

This lecture introduces **Observability** in LLM-based chatbot systems using **LangSmith**, a powerful monitoring and tracing tool for LangGraph applications. You'll learn how to track every interaction, measure performance metrics, and organize conversations efficiently.

### ğŸ¯ Prerequisites

- Basic understanding of LangGraph fundamentals
- Familiarity with chatbot development
- Knowledge of environment variables
- Understanding of threading concepts in chatbots
- Python programming basics

### ğŸ’¡ Why It Matters

Observability is critical for:
- **Debugging**: Understanding what went wrong in production
- **Performance Monitoring**: Tracking token usage, latency, and costs
- **User Behavior Analysis**: Understanding how users interact with your chatbot
- **System Optimization**: Identifying bottlenecks and improving response times
- **Compliance & Auditing**: Maintaining records of all interactions

---

## Journey Recap: Agentic AI Playlist

### ğŸ—ºï¸ Progress So Far

| Phase | Topics Covered | Outcome |
|-------|---------------|---------|
| **Theory** | What is Agentic AI? What is LangGraph? | Conceptual foundation |
| **Fundamentals** | LangGraph basics, workflow types | Core skills |
| **Project Start** | Built a basic chatbot | Working prototype |
| **Feature 1** | GUI implementation | User interaction capability |
| **Feature 2** | Streaming responses | Real-time user experience |
| **Feature 3** | Database persistence | Chat history preservation |
| **Feature 4 (Today)** | Observability with LangSmith | Production-ready monitoring |

### ğŸ¯ Current Chatbot Capabilities

```
âœ… GUI for user interaction
âœ… Streaming responses (no waiting)
âœ… Database persistence (chats survive restarts)
ğŸ†• Observability (today's addition)
```

---

## What is Observability?

### ğŸ“– Definition

> **Observability** is the ability to understand the internal state of a system by examining its outputs. In the context of LLM applications, it means tracking and recording every step of execution from user input to AI response.

### ğŸ” What Gets Tracked?

1. **User Messages**: Every input from the user
2. **AI Responses**: Complete outputs generated
3. **Token Usage**: Input tokens + Output tokens
4. **Latency Metrics**: 
   - Time to first token
   - Total response time
   - Execution start/end timestamps
5. **System Status**: Success/failure states
6. **Internal Execution**: How each component behaves

### ğŸ¯ Real-World Analogy

Think of observability like a **flight data recorder (black box)** in an airplane:
- Records every action during flight
- Helps understand what happened during incidents
- Provides insights for improving future flights
- Essential for debugging and optimization

---

## Why Observability Matters

### ğŸ¢ Production Scenarios

#### Scenario 1: Cost Management
```
Problem: Your LLM costs are skyrocketing
Solution: LangSmith shows token usage per conversation
Action: Identify which queries consume most tokens
Result: Optimize prompts to reduce costs by 40%
```

#### Scenario 2: Performance Issues
```
Problem: Users complain about slow responses
Solution: LangSmith tracks latency metrics
Action: Identify bottleneck in RAG retrieval step
Result: Optimize vector search, reduce latency by 60%
```

#### Scenario 3: Quality Assurance
```
Problem: Some users get irrelevant responses
Solution: Review traces to see exact conversation flow
Action: Identify pattern in failing queries
Result: Improve prompt engineering for edge cases
```

### ğŸ“Š Business Value

| Metric | Without Observability | With Observability |
|--------|----------------------|-------------------|
| **Debugging Time** | Hours/Days | Minutes |
| **Cost Visibility** | Unknown | Real-time tracking |
| **User Issues** | Guesswork | Data-driven insights |
| **Optimization** | Trial & error | Targeted improvements |

---

## Introduction to LangSmith

### ğŸ› ï¸ What is LangSmith?

**LangSmith** is a comprehensive platform for:
- **Tracing**: Recording execution flows
- **Monitoring**: Real-time performance metrics
- **Debugging**: Identifying issues in LLM applications
- **Testing**: Running experiments and evaluations
- **Prompt Management**: Version control for prompts

### ğŸŒ Official Resources

- **Website**: https://smith.langchain.com
- **Documentation**: Comprehensive guides available
- **Pricing**: Free tier available for development

### ğŸ¯ Key Features

1. **Automatic Tracing**: No code changes needed (basic setup)
2. **Thread Organization**: Group related conversations
3. **Rich Metadata**: Custom tags and information
4. **Visual Interface**: Beautiful dashboards
5. **Search & Filter**: Find specific traces quickly

---

## Setting Up LangSmith

### ğŸ“ Step 1: Create Account

1. Visit https://smith.langchain.com
2. Sign up for a free account
3. Verify your email
4. Log in to dashboard

### ğŸ”‘ Step 2: Generate API Key

```python
# Navigate to: Settings â†’ API Keys â†’ Create API Key

Steps:
1. Click "Settings" in sidebar
2. Select "API Keys"
3. Click "Create API Key" button
4. Provide a description (e.g., "Chatbot Project")
5. Click "Create"
6. COPY the API key immediately (shown only once)
```

### âš™ï¸ Step 3: Configure Environment Variables

Create or update your `.env` file:

```python
# .env file

# Enable LangSmith tracing
LANGCHAIN_TRACING_V2=true

# LangSmith API endpoint
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# Your API key (replace with actual key)
LANGCHAIN_API_KEY=your_api_key_here

# Project name for organizing traces
LANGCHAIN_PROJECT=chatbot-project
```

### ğŸ” Understanding Each Variable

| Variable | Purpose | Example Value |
|----------|---------|---------------|
| `LANGCHAIN_TRACING_V2` | Enables/disables tracing | `true` or `false` |
| `LANGCHAIN_ENDPOINT` | LangSmith server URL | `https://api.smith.langchain.com` |
| `LANGCHAIN_API_KEY` | Authentication token | `lsv2_pt_abc123...` |
| `LANGCHAIN_PROJECT` | Project name for organization | `chatbot-project` |

### âœ… Verification

Once configured, LangSmith automatically starts tracing your LangGraph application **without any code changes**.

```python
# No changes needed in your main code!
# Just run your application normally
# Traces will appear in LangSmith dashboard
```

---

## Understanding Traces

### ğŸ“Š Hierarchy in LangSmith

```
Project (Top Level)
â””â”€â”€ Traces (Individual executions)
    â””â”€â”€ Runs (Steps within execution)
        â””â”€â”€ Metadata (Details about each step)
```

### ğŸ¯ What is a Trace?

> A **trace** represents one complete interaction cycle: user sends message â†’ chatbot responds.

#### Example Trace Flow

```
User: "Give me a roadmap to study AI engineering"
  â†“
[TRACE STARTS]
  â†“
LangGraph processes request
  â†“
LLM generates response
  â†“
Response streamed to user
  â†“
[TRACE ENDS]
```

### ğŸ“ˆ Trace Information Captured

```python
{
    "trace_id": "unique_identifier",
    "node_name": "chat_node",
    "model": "ChatOpenAI",
    "input": "Give me a roadmap to study AI engineering",
    "output": "Here's a comprehensive roadmap...",
    "start_time": "2024-01-15T10:30:00Z",
    "end_time": "2024-01-15T10:30:05Z",
    "latency_ms": 5000,
    "time_to_first_token_ms": 800,
    "tokens": {
        "input": 12,
        "output": 450,
        "total": 462
    },
    "status": "success"
}
```

### ğŸ”„ Multiple Traces Example

```
Conversation Thread:
â”œâ”€â”€ Trace 1: "Hi" â†’ "Hello! How can I assist you?"
â”œâ”€â”€ Trace 2: "My name is Nitish" â†’ "Nice to meet you, Nitish!"
â””â”€â”€ Trace 3: "Who created you?" â†’ "I was created by OpenAI"

Each turn = One trace
```

---

## Implementing Thread-Based Organization

### âŒ Problem: Flat Trace Storage

**Without thread organization:**

```
All Traces (Mixed Together):
- Thread 1, Turn 1: "Hi"
- Thread 2, Turn 1: "Recipe of biryani"
- Thread 1, Turn 2: "My name is Nitish"
- Thread 2, Turn 2: "How to cook rice?"
- Thread 1, Turn 3: "Who created you?"

âŒ Confusing! Can't distinguish conversations
```

### âœ… Solution: Thread-Based Organization

**With thread organization:**

```
Thread 1 (AI Engineering Discussion):
â”œâ”€â”€ Turn 1: "Hi" â†’ "Hello!"
â”œâ”€â”€ Turn 2: "My name is Nitish" â†’ "Nice to meet you!"
â””â”€â”€ Turn 3: "Who created you?" â†’ "I was created by OpenAI"

Thread 2 (Cooking Discussion):
â”œâ”€â”€ Turn 1: "Recipe of biryani" â†’ "Here's the recipe..."
â””â”€â”€ Turn 2: "How to cook rice?" â†’ "Follow these steps..."

âœ… Clear separation of conversations!
```

### ğŸ¯ Benefits of Thread Organization

| Aspect | Without Threads | With Threads |
|--------|----------------|--------------|
| **Clarity** | All traces mixed | Organized by conversation |
| **Debugging** | Hard to follow flow | Easy to trace conversation |
| **Analysis** | Manual grouping needed | Automatic grouping |
| **User Experience** | Can't track user journey | Complete user journey visible |

---

## Code Implementation Deep Dive

### ğŸ“‹ Basic Setup (Automatic Tracing)

```python
# app.py - No changes needed for basic tracing!

from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

# Load environment variables (includes LangSmith config)
load_dotenv()

# Your existing chatbot code
# LangSmith automatically traces everything!

def chat_node(state):
    """Process user message and generate response"""
    messages = state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}

# Build and run graph as usual
# Traces appear in LangSmith automatically
```

**What happens behind the scenes:**

```python
# When you run your app:
1. Python loads .env file
2. LangSmith SDK detects LANGCHAIN_TRACING_V2=true
3. SDK intercepts all LangGraph/LangChain calls
4. Sends trace data to LangSmith API
5. Dashboard updates in real-time
```

### ğŸ§µ Advanced Setup (Thread Organization)

#### âŒ Old Config (Without Thread Metadata)

```python
# Old configuration - traces not organized by thread

config = {
    "configurable": {
        "thread_id": st.session_state.thread_id
    }
}

# Problem: LangSmith doesn't know about thread_id
# All traces stored flat in project
```

#### âœ… New Config (With Thread Metadata)

```python
# New configuration - traces organized by thread

from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    configurable={
        "thread_id": st.session_state.thread_id
    },
    metadata={
        "thread_id": st.session_state.thread_id,  # Tell LangSmith about thread
    },
    run_name="chat_turn"  # Custom name for better readability
)
```

### ğŸ” Line-by-Line Breakdown

```python
# Import the configuration class
from langchain_core.runnables import RunnableConfig

# Create configuration object
config = RunnableConfig(
    
    # Part 1: Configurable parameters (for LangGraph)
    configurable={
        "thread_id": st.session_state.thread_id
        # This tells LangGraph which thread to use for memory
        # Same as before - no change here
    },
    
    # Part 2: Metadata (for LangSmith) - NEW!
    metadata={
        "thread_id": st.session_state.thread_id
        # This tells LangSmith to organize traces by thread_id
        # Key must be "thread_id" or "conversation_id" or "session_id"
    },
    
    # Part 3: Custom run name (optional) - NEW!
    run_name="chat_turn"
    # Changes default "LangGraph" label to "chat_turn"
    # Makes traces more readable in dashboard
)
```

### ğŸ“Š Complete Working Example

```python
# complete_chatbot.py

import streamlit as st
from langgraph.graph import StateGraph, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
from dotenv import load_dotenv
import os

# Load environment variables (includes LangSmith config)
load_dotenv()

# Initialize LLM
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY")
)

# Define chat node
def chat_node(state: MessagesState):
    """
    Process messages and generate AI response
    
    Args:
        state: Current conversation state with messages
        
    Returns:
        Updated state with AI response
    """
    messages = state["messages"]
    response = llm.invoke(messages)
    return {"messages": [response]}

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("chat", chat_node)
builder.set_entry_point("chat")
builder.set_finish_point("chat")

# Add memory
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# Streamlit UI
st.title("AI Chatbot with LangSmith Observability")

# Initialize session state
if "thread_id" not in st.session_state:
    st.session_state.thread_id = "default_thread"
    
if "messages" not in st.session_state:
    st.session_state.messages = []

# Sidebar for thread management
with st.sidebar:
    st.header("Thread Management")
    
    # Create new thread button
    if st.button("New Conversation"):
        import uuid
        st.session_state.thread_id = str(uuid.uuid4())
        st.session_state.messages = []
        st.rerun()
    
    # Display current thread ID
    st.text(f"Current Thread: {st.session_state.thread_id[:8]}...")

# Display chat history
for message in st.session_state.messages