---
title: "NoteSmith - Lecture Notes: MCP (Model Context Protocol) in LangGraph"
layout: default
nav_order: 17
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes: MCP (Model Context Protocol) in LangGraph"
last_modified_date: 2026-01-18
source_transcript: "019_How to build MCP Client using LangGraph _ Agentic AI using LangGraph _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes: MCP (Model Context Protocol) in LangGraph

---

## Table of Contents

1. [Overview](#overview)
2. [Playlist Recap](#playlist-recap)
3. [Understanding MCP (Model Context Protocol)](#understanding-mcp)
4. [The Problem with Traditional Tools](#the-problem-with-traditional-tools)
5. [How MCP Solves the Problem](#how-mcp-solves-the-problem)
6. [Converting Synchronous Code to Asynchronous](#converting-synchronous-code-to-asynchronous)
7. [Building an MCP Client in LangGraph](#building-an-mcp-client-in-langgraph)
8. [Connecting Multiple MCP Servers](#connecting-multiple-mcp-servers)
9. [Integrating MCP with Existing Chatbot](#integrating-mcp-with-existing-chatbot)
10. [Quick Reference](#quick-reference)
11. [Summary Table](#summary-table)
12. [Key Takeaways](#key-takeaways)
13. [Edge Cases & Common Mistakes](#edge-cases--common-mistakes)
14. [Interview Questions](#interview-questions)

---

## Overview

### ðŸ“š What This Covers
This lecture continues the **Agentic AI using LangGraph** series, focusing on integrating **MCP (Model Context Protocol)** into LangGraph chatbots. You'll learn why MCP is superior to traditional tools, how to convert synchronous code to asynchronous, and how to build production-ready MCP clients.

### ðŸŽ¯ Prerequisites
- Basic understanding of LangGraph concepts (nodes, edges, state)
- Familiarity with Python async/await
- Knowledge of chatbot architecture
- Understanding of tools in LangGraph (from previous videos)
- **Recommended**: Complete the instructor's MCP playlist (8 videos) for deep understanding

### ðŸ’¡ Why This Matters
- **Industry Standard**: MCP is becoming the standard way to connect tools to LLM applications (even ChatGPT is implementing it)
- **Maintenance**: Reduces maintenance burden from O(nÃ—m) to O(1) where n=tools, m=chatbots
- **Future-Proof**: Server-side changes don't require client-side code updates
- **Scalability**: Easily add new capabilities without rewriting code

---

## Playlist Recap

### ðŸŽ¬ Previous Topics Covered (18 Videos)

| Topic Category | Content Covered |
|---------------|-----------------|
| **Fundamentals** | What is Agentic AI, Difference between Generative AI and Agentic AI, LangGraph vs LangChain |
| **Core Concepts** | LangGraph core concepts and architecture |
| **Workflows** | Parallel, Sequential, Conditional, Iterative workflows |
| **Chatbot Project** | Basic chatbot â†’ UI â†’ Streaming â†’ Resume chat â†’ Database â†’ Observability (LangSmith) â†’ Tools integration |
| **Tools** | Stock market tool, Web search tool, Calculator tool |

### Current Status
The chatbot currently uses **traditional tools** approach. Today we'll upgrade it to use **MCP**.

---

## Understanding MCP

### ðŸ” What is MCP?

> **MCP (Model Context Protocol)** is a standardized way to connect tools to LLM applications. It's an improved version of the traditional tools approach that provides better separation of concerns between client and server.

### Simple Definition
```
MCP = Improved Version of Tools
```

**Key Difference:**
- **Tools**: Tightly coupled code on client side
- **MCP**: Separation of concerns - server handles logic, client handles configuration

### Why MCP Exists

MCP solves the **inherent flaw** in traditional tools: **brittleness** and **maintenance overhead**.

---

## The Problem with Traditional Tools

### ðŸš¨ The Brittleness Problem

#### Example Scenario

**Current Setup:**
```
Chatbot (Client) â†â†’ GitHub API (Server)
```

**Traditional Tool Code:**
```python
@tool
def get_pull_requests(owner: str, repo_url: str, state: str = "open", limit: int = 5):
    """Fetch pull requests from GitHub repository"""
    
    # GitHub access token
    token = os.getenv("GITHUB_TOKEN")
    
    # Headers
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    # API endpoint
    url = f"https://api.github.com/repos/{owner}/{repo}/pulls"
    
    # Make request
    response = requests.get(url, headers=headers, params={"state": state})
    data = response.json()
    
    # Extract information
    results = []
    for pr in data[:limit]:
        results.append({
            "number": pr["number"],
            "title": pr["title"],
            "author": pr["user"]["login"],
            "state": pr["state"],
            "url": pr["html_url"]
        })
    
    return results
```

#### What Happens When GitHub Updates?

**Scenario:** GitHub releases API v2.0 with breaking changes:
- URL changes: `/pulls` â†’ `/pull_requests`
- Field names change: `title` â†’ `title_name`, `user` â†’ `user_name`

**Impact:**
```
âŒ Your chatbot breaks immediately
âŒ You must update ALL tool code
âŒ If you have 10 tools for GitHub â†’ update 10 places
âŒ If you have 5 chatbots â†’ update 5Ã—10 = 50 places
âŒ If you integrate Gmail, Slack, Jira â†’ nÃ—m maintenance problem
```

### The nÃ—m Maintenance Problem

| Scenario | Tools | Chatbots | Update Points |
|----------|-------|----------|---------------|
| Small | 3 | 1 | 3 |
| Medium | 10 | 5 | 50 |
| Large | 20 | 10 | 200 |

**You spend all your time maintaining code instead of building features.**

---

## How MCP Solves the Problem

### ðŸŽ¯ Separation of Concerns

MCP introduces a **clear separation** between:
- **Server Side**: All tool logic and implementation
- **Client Side**: Only configuration code

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Client     â”‚         â”‚   MCP Server     â”‚
â”‚  (Your Chatbot) â”‚ â†â”€â”€â”€â”€â†’  â”‚   (GitHub)       â”‚
â”‚                 â”‚         â”‚                  â”‚
â”‚  Config Only    â”‚         â”‚  All Tool Logic  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Traditional Tools vs MCP

#### Traditional Approach
```python
# Client Side - Heavy lifting code (100+ lines)
@tool
def get_pull_requests(...):
    # All implementation here
    # API calls
    # Data parsing
    # Error handling
    pass

# When GitHub updates API â†’ Must change this code
```

#### MCP Approach
```python
# Client Side - Configuration only (5 lines)
client = MultiServerMCPClient({
    "github": {
        "transport": "stdio",
        "command": "python",
        "args": ["/path/to/github_mcp_server/main.py"]
    }
})

# When GitHub updates API â†’ No changes needed here!
# Server handles all updates
```

### Key Benefits

| Aspect | Traditional Tools | MCP |
|--------|------------------|-----|
| **Code Location** | Client-side (your chatbot) | Server-side (separate) |
| **Maintenance** | Update every chatbot | Update only server |
| **Coupling** | Tightly coupled | Loosely coupled |
| **Scalability** | O(nÃ—m) updates | O(1) updates |
| **Future-proof** | âŒ Breaks on API changes | âœ… Resilient to changes |

---

## Converting Synchronous Code to Asynchronous

### ðŸ”„ Why Async is Required

> **Important**: The MCP client library (`langchain-mcp-adapters`) only works in **asynchronous mode**. Therefore, we must convert our synchronous LangGraph code to async.

### Understanding Async Programming

#### What is Asynchronous Execution?

**Synchronous (Sequential):**
```python
# Task 1 completes â†’ Then Task 2 starts â†’ Then Task 3 starts
result1 = fetch_weather()      # Wait 2 seconds
result2 = fetch_cricket_score() # Wait 2 seconds
# Total time: 4 seconds
```

**Asynchronous (Parallel):**
```python
# Task 1 and Task 2 run simultaneously
result1 = await fetch_weather()      # 2 seconds
result2 = await fetch_cricket_score() # 2 seconds (parallel)
# Total time: 2 seconds
```

#### Real-World Example

**User Query:** "Find me the weather of Bangalore where the match is happening and also tell me the current score of the match."

**Synchronous:**
1. Fetch weather (wait)
2. Then fetch cricket score (wait)
3. Return results
**Total: 4 seconds**

**Asynchronous:**
1. Fetch weather (parallel)
2. Fetch cricket score (parallel)
3. Return results when both complete
**Total: 2 seconds**

### Step-by-Step Conversion

#### Original Synchronous Code

```python
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# Tool definition
@tool
def calculator(operation: str, a: float, b: float) -> float:
    """Perform mathematical operations"""
    if operation == "add":
        return a + b
    elif operation == "multiply":
        return a * b
    # ... more operations

# LLM with tools
llm = ChatOpenAI(model="gpt-4")
llm_with_tools = llm.bind_tools([calculator])

# State
class State(TypedDict):
    messages: Annotated[list, add_messages]

# Nodes
def chat_node(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# Build graph
graph = StateGraph(State)
graph.add_node("chat", chat_node)
graph.add_node("tools", ToolNode([calculator]))
graph.add_edge(START, "chat")
graph.add_conditional_edges("chat", should_continue)
graph.add_edge("tools", "chat")
chatbot = graph.compile()

# Run
result = chatbot.invoke({"messages": [("user", "What is 5 + 3?")]})
```

#### Converted Asynchronous Code

```python
import asyncio
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# Tool definition (same as before)
@tool
def calculator(operation: str, a: float, b: float) -> float:
    """Perform mathematical operations"""
    if operation == "add":
        return a + b
    elif operation == "multiply":
        return a * b

# LLM with tools (same as before)
llm = ChatOpenAI(model="gpt-4")
llm_with_tools = llm.bind_tools([calculator])

# State (same as before)
class State(TypedDict):
    messages: Annotated[list, add_messages]

# Async node - KEY CHANGE
async def chat_node(state: State):
    # Use ainvoke instead of invoke
    response = await llm_with_tools.ainvoke(state["messages"])
    return {"messages": [response]}

# Build graph function
async def build_graph():
    graph = StateGraph(State)
    graph.add_node("chat", chat_node)  # Now async
    graph.add_node("tools", ToolNode([calculator]))  # Already async internally
    graph.add_edge(START, "chat")
    graph.add_conditional_edges("chat", should_continue)
    graph.add_edge("tools", "chat")
    return graph.compile()

# Main async function
async def main():
    chatbot = await build_graph()
    
    # Use ainvoke instead of invoke
    result = await chatbot.ainvoke({
        "messages": [("user", "What is 5 + 3?")]
    })
    
    print(result)

# Run the async code
if __name__ == "__main__":
    asyncio.run(main())
```

### Key Changes Summary

| Component | Synchronous | Asynchronous |
|-----------|-------------|--------------|
| **Function Declaration** | `def chat_node(state):` | `async def chat_node(state):` |
| **LLM Invocation** | `llm.invoke(...)` | `await llm.ainvoke(...)` |
| **Graph Building** | Direct call | `async def build_graph():` |
| **Chatbot Invocation** | `chatbot.invoke(...)` | `await chatbot.ainvoke(...)` |
| **Execution** | Direct call | `asyncio.run(main())` |
| **Import** | Not needed | `import asyncio` |

### Important Notes

1. **ToolNode is already async**: You don't need to make ToolNode async - it's internally asynchronous
2. **Only custom nodes need async**: Convert your custom nodes (like `chat_node`) to async
3. **await keyword**: Use `await` before any async function call
4. **asyncio.run()**: Entry point for async programs

---

## Building an MCP Client in LangGraph

### ðŸ› ï¸ Setup and Installation

#### Install Required Library

```bash
# Using pip
pip install langchain-mcp-adapters

# Using uv (recommended)
uv add langchain-mcp-adapters
```

### MCP Server Overview

Before building the client, understand what we're connecting to:

#### Example: Math MCP Server

```python
# math_mcp_server/main.py
from mcp import Server

# This server provides 6 mathematical tools:
# 1. add(a, b) - Addition
# 2. subtract(a, b) - Subtraction
# 3. multiply(a, b) - Multiplication
# 4. divide(a, b) - Division
# 5. power(a, b) - Exponentiation
# 6. modulus(a, b) - Modulus operation

# All tools are async functions
async def add(a: float, b: float) -> float:
    return a + b

async def subtract(a: float, b: float) -> float:
    return a - b

# ... more tools
```

**Server Location:** `/Users/username/Desktop/mcp_math_server/main.py`

### Building the MCP Client

#### Step 1: Import MCP Client

```python
from langchain_mcp_adapters.client import MultiServerMCPClient
import asyncio
```

#### Step 2: Create Client Configuration

```python
# Create MCP client instance
client = MultiServerMCPClient({
    "math_server": {  # Server name (can be anything)
        "transport": "stdio",  # Transport type for local servers
        "command": "python",   # Command to run the server
        "args": ["/Users/username/Desktop/mcp_math_server/main.py"]  # Server path
    }
})
```

**Configuration Breakdown:**

| Parameter | Value | Explanation |
|-----------|-------|-------------|
| `"math_server"` | Server identifier | Any name you choose |
| `"transport"` | `"stdio"` | Standard Input/Output (for local servers) |
| `"command"` | `"python"` | How to execute the server |
| `"args"` | `["/path/to/server"]` | Full path to server file |

#### Step 3: Fetch Tools from Server

```python
async def build_graph():
    # Fetch all tools from the MCP server
    tools = await client.get_tools()
    
    # Print available tools (optional)
    print("Available tools:", tools)
    
    # Bind tools to LLM
    llm_with_tools = llm.bind_tools(tools)
    
    # Continue building graph...
```

#### Complete MCP Client Code

```python
import asyncio
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.client import MultiServerMCPClient