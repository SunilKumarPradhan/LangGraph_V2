---
title: "Complete Guide: Long-Term Memory Implementation in LangGraph for Agentic AI"
layout: default
nav_order: 23
parent: "Lecture Notes"
description: "Lecture notes: Complete Guide: Long-Term Memory Implementation in LangGraph for Agentic AI"
last_modified_date: 2026-01-18
source_transcript: "025_Long Term Memory in LangGraph"
generated_by: "NoteSmith"
---

# Complete Guide: Long-Term Memory Implementation in LangGraph for Agentic AI

## Course Context
This lecture is part of the Agentic AI using LangGraph playlist, focusing on building production-grade chatbot systems with persistent memory capabilities.

**Previous Coverage:**
- Memory foundations (theoretical concepts)
- Short-term memory implementation
- **Current Focus:** Long-term memory implementation

---

## 1. Understanding Long-Term Memory

### The Core Problem: Information Fragmentation

**Multi-threaded Conversation Scenario:**
Users typically create separate conversation threads for different topics (similar to ChatGPT):
- **Thread 1:** Technical discussions (e.g., LangChain programming)
- **Thread 2:** Travel planning (e.g., Mumbai trip)
- **Thread 3:** Personal/philosophical thoughts

**The Challenge:**
User information becomes scattered across conversations:
- Technical thread reveals: User is a programmer, prefers Python
- Travel thread reveals: Planning to move to Mumbai
- Personal thread reveals: Philosophical/spiritual inclinations

### The Solution: Centralized Memory Store

**Key Components:**
1. **Memory Store** - Database-like persistent storage
2. **Information Extraction** - Automated extraction from user queries
3. **Cross-conversation Access** - Memories available across all threads
4. **Personalization Engine** - Uses stored memories for contextual responses

**Process Flow:**
```
User Query → Answer Query + Extract User Info → Store in Memory Database
                                                        ↓
                                              Persistent Storage
                                                        ↓
                                    Accessible Across All Conversations
```

**Personalization Example:**
- **Stored Memory:** "User prefers Python"
- **User Query:** "Write code for Fibonacci series"
- **LLM Process:**
  1. Check memory store
  2. Find programming language preference
  3. Generate Python code (personalized response)

---

## 2. Technical Architecture

### BaseStore Class Hierarchy

LangGraph implements memory through an abstract class structure:

```
BaseStore (Abstract Class)
    ├── InMemoryStore (RAM storage - prototyping only)
    ├── PostgresStore (Production - PostgreSQL database)
    └── RedisStore (Production - Redis database)
```

**BaseStore Capabilities:**
- **Create** new memories
- **Search** existing memories
- **Edit** existing memories
- **Delete** existing memories

### Store Implementation Options

#### 1. InMemoryStore
- **Storage:** RAM (volatile)
- **Persistence:** None (lost on restart)
- **Use Case:** Quick prototyping and testing
- **Production:** ❌ Not recommended

#### 2. PostgresStore
- **Storage:** PostgreSQL database
- **Persistence:** ✅ Permanent
- **Use Case:** Production deployments
- **Production:** ✅ Recommended

#### 3. RedisStore
- **Storage:** Redis database
- **Persistence:** ✅ Permanent
- **Use Case:** Production deployments (fast access)
- **Production:** ✅ Recommended

---

## 3. Working with Memory Stores

### Setup and Initialization

```python
from langgraph.store.memory import InMemoryStore

# Create store object
store = InMemoryStore()
```

### Understanding Namespaces

**Concept:** Namespaces organize memories like folders in a file system

**Structure:** Tuples of strings creating hierarchical organization

**Examples:**
```python
("users", "U1")                    # All memories for User 1
("users", "U2")                    # All memories for User 2
("users", "U1", "profile")         # User 1's profile information
("users", "U2", "profile")         # User 2's profile information
("users", "U1", "preferences")     # User 1's preferences
```

**Key Points:**
- Namespaces are **tuples** (immutable sequences)
- Support hierarchical nesting
- Enable organized memory categorization

### Creating Memories: PUT Method

**Syntax:**
```python
store.put(namespace, key, value)
```

**Parameters:**
- `namespace`: Organizational folder (tuple)
- `key`: Unique identifier (string)
- `value`: Memory content (dictionary)

**Example Implementation:**
```python
# Define namespace
namespace = ("users", "U1")

# Add memories
store.put(
    namespace,
    "1",
    {"data": "User likes pizza and this is our first memory"}
)

store.put(
    namespace,
    "2",
    {"data": "User prefers dark mode"}
)

# Create memories for second user
namespace_2 = ("users", "U2")
store.put(namespace_2, "1", {"data": "User likes pasta"})
store.put(namespace_2, "2", {"data": "User prefers grid style navigation"})
```

### Retrieving Specific Memories: GET Method

**Purpose:** Retrieve a single memory when you know the exact key

**Syntax:**
```python
store.get(namespace, key)
```

**Examples:**
```python
# Get User 1's first memory
store.get(("users", "U1"), "1")
# Returns: "User likes pizza"

# Get User 2's second memory
store.get(("users", "U2"), "2")
# Returns: "User prefers grid style navigation"
```

### Retrieving All Memories: SEARCH Method

**Purpose:** Retrieve ALL memories from a namespace

**Syntax:**
```python
store.search(namespace)
```

**Example:**
```python
# Get all memories for User 1
items = store.search(("users", "U1"))

# Iterate through results
for item in items:
    print(item)
# Output: All User 1's memories

# Get all memories for User 2
items = store.search(("users", "U2"))
for item in items:
    print(item)
# Output: All User 2's memories
```

---

## 4. Semantic Search Implementation

### The Problem with Basic Retrieval

**Three Scenarios:**
1. **GET:** You know the exact key → Retrieve specific memory
2. **SEARCH:** You want everything → Retrieve all memories
3. **❓ Gap:** You need relevant memories but don't know which ones

### Real-World Challenge

**Scenario:**
- User has 100 stored memories
- Currently discussing Mumbai travel plans
- Only 2-3 memories relate to Mumbai
- Fetching all 100 → LLM gets confused with irrelevant data
- **Solution Needed:** Fetch only Mumbai-related memories

### Semantic Search Concept

**Definition:** Search based on **meaning** rather than exact keyword matches

**Process:**
1. Analyze current conversation context
2. Match semantic meaning with stored memories
3. Retrieve only contextually relevant memories

### Implementation Steps

#### Step 1: Create Store with Embedding Model

```python
from langchain_openai import OpenAIEmbeddings

# Initialize embedding model
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# Create store with semantic search capability
store = InMemoryStore(
    index={
        "embedding": embedding_model,
        "dims": 1536  # embedding dimensions for text-embedding-3-small
    }
)
```

#### Step 2: Create Memories (Standard Process)

```python
namespace = ("users", "U1")

# Create diverse memories
store.put(namespace, "1", {"data": "User's name is Nitesh"})
store.put(namespace, "2", {"data": "User is a programmer"})
store.put(namespace, "3", {"data": "User prefers Python"})
store.put(namespace, "4", {"data": "User likes dark mode"})
store.put(namespace, "5", {"data": "User is learning machine learning"})
# ... additional memories
```

#### Step 3: Semantic Search with Query

```python
# Search with natural language query
items = store.search(
    namespace,
    query="What is user currently learning?",
    limit=1  # Return top matching memory
)

# Result: Returns "User is learning machine learning"
# (semantically matches the query)
```

**Key Advantage:** Retrieves contextually relevant memories without exact keyword matching

---

## 5. Memory Integration in Chat Systems

### System Prompt Integration Process

**Workflow:**
1. Retrieve all relevant memories (string format)
2. Take system prompt template
3. Apply format function to inject memories
4. Fill user detail content with retrieved memories
5. Convert to system message
6. Pass to LLM with conversation history

**Critical Concept:** Long-term memories are injected into short-term memory by passing them through the LLM's context window

### Basic Chat Graph Structure

```
START → CHAT → END
```

**Configuration:**
- Connected to memory store
- User ID: U1 (example)
- Query: "Explain Gen AI in simple terms"

### Personalization in Action

**Example Response Analysis:**

**User Query:** "Explain Gen AI in simple terms"

**LLM Response Demonstrates:**
- Addressed user as "Sure Nitish" (name retrieved from memory)
- Asked Python-specific follow-up questions (preferred language from memory)
- Suggested teaching materials integration (user is YouTube teacher from memory)

**Conclusion:** Long-term memories enable highly personalized, contextual responses

---

## 6. Creating New Memories

### Memory Creation Workflow

```
START → REMEMBER → END
```

**Important:** This workflow ONLY creates memories; it doesn't use existing ones or provide chat responses

### Implementation Components

#### 1. Memory Extractor LLM
- Dedicated LLM for memory extraction
- Separate from chat LLM
- Decides what information is worth storing

#### 2. Pydantic Model: MemoryDecision

```python
class MemoryDecision:
    should_write: bool      # True if memory worth storing
    memories: List[str]     # List of extractable memories
```

#### 3. Structured Output
- Uses `with_structured_output()` method
- Ensures consistent, parseable LLM responses
- Enforces Pydantic model schema

### Remember Node Logic

**Process Flow:**
1. Extract user ID from configuration
2. Form namespace tuple
3. Extract last message from state
4. Invoke memory extractor LLM with:
   - System message (extraction instructions)
   - User's last message
5. Check `should_write` flag
6. If true, iterate through memories list
7. Store each memory using `store.put()`

### System Prompt for Memory Extraction

**Key Instructions:**
- Extract long-term memories from user messages
- Store only stable, user-specific information:
  - Identity details
  - Preferences
  - Ongoing projects
- **Don't store:**
  - Transient information
  - Temporary states
  - One-time queries
- Return `should_write = False` if nothing worth storing
- Each memory should be short and atomic (one fact per memory)

---

## 7. Solving Memory Duplication

### The Duplication Problem

**Demonstration:**
- Sending "My name is Nitish" twice creates two identical entries
- No deduplication mechanism in basic implementation
- **Root Cause:** System doesn't check for existing similar memories

### Enhanced Deduplication Strategy

#### Updated Pydantic Models

**MemoryItem Model:**
```python
class MemoryItem:
    text: str           # The memory content
    is_new: bool        # True = new memory, False = existing/duplicate
```

**Updated MemoryDecision Model:**
```python
class MemoryDecision:
    should_write: bool
    memories: List[MemoryItem]  # Now list of MemoryItem objects
```

### Enhanced Extraction Process

**Workflow:**
1. Send user's latest message + existing memories to LLM
2. LLM extracts potential memories
3. For each extracted memory:
   - Compare against current user details
   - Set `is_new = True` only if adds new information
   - Set `is_new = False` if essentially duplicates existing memory
4. Return structured decision

### Updated System Prompt

**Additional Instructions:**
- Review user's latest message
- Extract user-specific information worth storing long-term
- **For each extracted item:**
  - Set `is_new = True` ONLY if it adds new information compared to current user details
  - Set `is_new = False` if it has basically the same meaning as something already present
- Compare semantic meaning, not just exact text

### Storage Logic with Deduplication

```python
for memory in decision.memories:
    if memory.is_new:  # Only store if genuinely new
        store.put(namespace, unique_key, memory.text)
```

### Testing Results

**Test:** Sent same messages twice
- **Result:** Memory store showed only 2 unique entries (no duplicates)
- **Conclusion:** ✅ Deduplication problem solved

---

## 8. Complete Chatbot Implementation

### Merged Workflow Architecture

```
START → REMEMBER → CHAT → END
```

**Remember Node:**
- Extracts memories from user's recent message
- Writes new memories to store
- Implements deduplication

**Chat Node:**
- Retrieves relevant existing memories
- Generates personalized responses
- Provides contextual answers

### Dual LLM Setup

1. **Memory Extractor LLM:** Manages memory extraction and storage
2. **Chat LLM:** Handles conversation and user responses

### Code Structure

**Components:**
- System prompts (extraction + chat)
- Pydantic models (with deduplication)
- Remember node implementation
- Chat node implementation
- Graph: `REMEMBER → CHAT → END`

### Demonstration Results

#### Message 1: "Hi, my name is Nitish"
- **Response:** "Hi Nitish, it's great to meet you..."
- **Memory Stored:** "User name is Nitish"

#### Message 2: "I teach AI on YouTube"
- **Response:** "That's great Nitish, teaching AI on YouTube sounds exciting..."
- **Follow-up:** Questions about teaching topics
- **Memory Stored:** "Nitish teaches AI on YouTube"

#### Message 3: "Explain Gen AI simply"
- **Response:** 
  - Addressed as "Sure Nitish"
  - Bullet-point explanation
  - Follow-up questions related to teaching
- **Memory Stored:** None (nothing new worth storing)

**Analysis:** System successfully:
- Extracts relevant memories
- Avoids duplicates
- Personalizes responses
- Decides when NOT to store information

---

## 9. Production-Grade Storage with PostgreSQL

### Critical Flaw: InMemoryStore

**Problem:**
- Stores all memories in RAM (volatile memory)
- **Major Issue:** All memories lost on restart/shutdown
- **Demonstration:** Restarting kernel → all memories disappeared

**Conclusion:** InMemoryStore is NOT suitable for production

### PostgreSQL Store Solution

**Benefits:**
- Persistent storage (survives restarts)
- Production-grade reliability
- Scalable for large memory volumes

### Setup Requirements

#### Step 1: Install Docker

```bash
# Verify installation
docker --version
```

**Instructions:**
1. Search "Docker Desktop"
2. Download for your operating system
3. Install and start Docker

#### Step 2: Run PostgreSQL Container

```bash
docker run -d \
  --name langgraph-postgres \
  -e POSTGRES_PASSWORD=password \
  -p 5432:5432 \
  postgres:16
```

**What This Does:**
- Downloads PostgreSQL image (if not present)
- Runs as background container
- Exposes port 5432
- Sets password to "password"

**Verification:**
```bash
docker ps  # Shows running containers
```

#### Step 3: Install Required Libraries

```bash
pip install psycopg2-binary langgraph-checkpoint-postgres
```

### Code Implementation

#### Database Connection

```python
from langgraph.checkpoint.postgres import PostgresSaver

# Connection string format
connection_string = "postgresql://user:password@localhost:5432/dbname"

# Use context manager for proper connection handling
with PostgresSaver.from_conn_string(connection_string) as store:
    # Initialize database tables
    store.setup()
    
    # Compile graph with PostgreSQL store
    graph = builder.compile(store=store)
    
    # Rest of code remains identical to InMemoryStore version
    # ... (same nodes, same logic)
```

### Key Differences from InMemoryStore

**Changes Required:**
1. **Connection String:** Specify PostgreSQL database URL
2. **Context Manager:** Use `with` statement for proper resource management
3. **Setup Call:** Execute `store.setup()` to initialize database schema

**Everything Else:** Identical code (same nodes, same logic, same functionality)

### Testing Persistence

**Test Procedure:**
1. Run code and store memories
2. Restart kernel (simulates system restart)
3. Run only memory retrieval code
4. **Result:** ✅ Memories still present!

**Proof:** PostgreSQL store provides true persistent memory storage

---

## 10. Production Best Practices

### Memory Store Selection

**For Production:**
1. **PostgreSQL** ✅ (demonstrated in this lecture)
2. **Redis** ✅ (alternative option)

**Never Use:**
- InMemoryStore ❌ (prototyping only)

### Implementation Checklist

✅ **Always use persistent storage**
- PostgreSQL or Redis
- Never rely on RAM-based storage

✅ **Test persistence**
- Restart system
- Verify memories survive

✅ **Implement deduplication**
- Use `is_new` flag strategy
- Compare semantic meaning

✅ **Use separate LLMs**
- Memory extraction LLM
- Chat/response LLM

✅ **Organize with namespaces**
- Hierarchical structure
- Clear categorization

✅ **Implement semantic search**
- Use embedding models
- Retrieve contextually relevant memories

---

## 11. Complete System Summary

### Full Production Workflow

**User Interaction Flow:**
1. User sends message
2. **Remember Node:**
   - Extracts new memories (if any)
   - Checks for duplicates
   - Stores only new information
3. **Chat Node:**
   - Retrieves relevant existing memories
   - Generates personalized response
   - Returns contextual answer
4. Memories persist in PostgreSQL database

### System Architecture Components

**Memory Store (PostgreSQL):**
- Persistent database storage
- Survives system restarts
- Scalable for production

**Dual LLM System:**
- Memory Extractor LLM (extraction + deduplication)
- Chat LLM (conversation + personalization)

**Namespace Organization:**
- Hierarchical memory structure
- User-specific categorization
- Efficient retrieval

**Semantic Search:**
- Embedding-based retrieval
- Context-aware memory selection
- Reduces irrelevant information

### Key Capabilities

✅ **Personalization:** Responses tailored to user profile
✅ **Persistence:** Memories survive restarts
✅ **Deduplication:** No redundant information
✅ **Scalability:** Production-grade database
✅ **Context-Awareness:** Semantic memory retrieval
✅ **Cross-Thread Access:** Memories available across all conversations

---

## Conclusion

This implementation provides a complete, production-ready long-term memory system for LangGraph chatbots. By combining persistent storage, semantic search, deduplication strategies, and dual LLM architecture, the system delivers highly personalized, contextual conversations that maintain user information across sessions and conversation threads.

**Next Steps:**
- Implement in your own projects
- Experiment with Redis as alternative storage
- Extend namespace hierarchies for complex use cases
- Fine-tune memory extraction prompts for your domain