---
title: "NoteSmith - Lecture Notes Generator"
layout: default
nav_order: 8
parent: "Lecture Notes"
description: "Lecture notes: NoteSmith - Lecture Notes Generator"
last_modified_date: 2026-01-18
source_transcript: "009_Iterative Workflows in LangGraph _ Agentic AI using LangGraph _ Video 8 _ CampusX"
generated_by: "NoteSmith"
---

# NoteSmith - Lecture Notes Generator

# Iterative Workflows in LangGraph: Building an AI-Powered Tweet Generator

---

## Table of Contents

1. [Overview](#overview)
2. [Workflow Types Recap](#workflow-types-recap)
3. [Understanding Iterative Workflows](#understanding-iterative-workflows)
4. [Real-World Use Case: Automated Tweet Generation](#real-world-use-case-automated-tweet-generation)
5. [Architecture Design](#architecture-design)
6. [Implementation Guide](#implementation-guide)
   - [Setting Up LLMs](#setting-up-llms)
   - [Defining State](#defining-state)
   - [Building Nodes](#building-nodes)
   - [Creating Edges and Loops](#creating-edges-and-loops)
7. [Code Walkthrough](#code-walkthrough)
8. [Quick Reference](#quick-reference)
9. [Summary Table](#summary-table)
10. [Key Takeaways](#key-takeaways)
11. [Edge Cases & Common Mistakes](#edge-cases--common-mistakes)
12. [Interview Questions](#interview-questions)

---

## ğŸ“‹ Overview

### What This Covers
This lecture introduces **Iterative (Looping) Workflows** in LangGraph through a practical project: building an automated tweet generation system with quality control. You'll learn how to create workflows that improve outputs through repeated evaluation and optimization cycles.

### Prerequisites
- Basic understanding of LangGraph
- Familiarity with Sequential, Parallel, and Conditional workflows
- Python programming knowledge
- Understanding of LLMs (Large Language Models)
- LangChain basics

### Why It Matters
Iterative workflows are crucial for:
- **Quality assurance** in AI-generated content
- **Self-improving systems** that refine outputs
- **Production-grade applications** requiring reliability
- **Complex agentic AI systems** that need feedback loops

**Real-world applications:**
- Content generation with quality gates
- Code review and optimization systems
- Data validation pipelines
- Automated testing frameworks
- Customer service chatbots with response refinement

---

## ğŸ”„ Workflow Types Recap

Before diving into iterative workflows, let's review what we've learned:

### 1. **Sequential Workflows**
Tasks execute one after another in linear fashion.
```
Task 1 â†’ Task 2 â†’ Task 3 â†’ ... â†’ Task N
```

### 2. **Parallel Workflows**
Multiple tasks execute simultaneously.
```
        â”Œâ”€ Task 2 â”€â”
Task 1 â”€â”¼â”€ Task 3 â”€â”¼â”€ Task 5
        â””â”€ Task 4 â”€â”˜
```

### 3. **Conditional Workflows**
Different tasks execute based on conditions.
```
           â”Œâ”€ Task A (if condition 1)
Task 1 â”€â”€â”€â”€â”¼â”€ Task B (if condition 2)
           â””â”€ Task C (if condition 3)
```

### 4. **Iterative Workflows** â­ (Today's Topic)
Tasks loop between nodes to improve results.
```
Generate â†’ Evaluate â‡„ Optimize
              â†“
            Approve
```

---

## ğŸ” Understanding Iterative Workflows

### What is an Iterative Workflow?

> **Iterative Workflow**: A workflow pattern where tasks loop between two or more nodes repeatedly to improve or refine an output until it meets specified criteria.

### Core Concept
Instead of accepting the first output from an LLM, iterative workflows:
1. **Generate** an initial output
2. **Evaluate** against quality criteria
3. **Optimize** based on feedback
4. **Repeat** steps 2-3 until approved or max iterations reached

### Why Use Iterative Workflows?

**Problem**: LLMs often produce mediocre outputs on the first attempt
- Generic, repetitive content
- Doesn't meet specific quality standards
- Lacks originality or punch

**Solution**: Implement a feedback loop
- Automated quality control
- Continuous improvement
- Consistent output quality
- No manual intervention needed

---

## ğŸ¯ Real-World Use Case: Automated Tweet Generation

### The Problem Statement

**Scenario**: As a YouTuber, maintaining presence across multiple platforms (LinkedIn, X/Twitter, Instagram) is time-consuming. Manual posting on each platform takes away from content creation time.

**Challenge**: How to automate social media posting while ensuring quality content?

**Risk**: Automated solutions might generate:
- Mediocre, generic posts
- Repetitive content
- Low engagement material
- Content that doesn't provide value

### The Solution

Build an **iterative workflow** that:
1. Generates tweets on given topics
2. Evaluates them against strict quality criteria
3. Optimizes rejected tweets based on feedback
4. Loops until quality standards are met
5. (Future) Implements human-in-the-loop approval
6. (Future) Auto-posts via API

---

## ğŸ—ï¸ Architecture Design

### System Components

Our workflow consists of **three main components**:

#### 1. **Generator** ğŸ¨
- **Role**: Creates initial tweet
- **Input**: Topic
- **Output**: Generated tweet
- **LLM Choice**: Model with strong writing capabilities (GPT-4o)

#### 2. **Evaluator** âš–ï¸
- **Role**: Judges tweet quality
- **Input**: Generated tweet + evaluation criteria
- **Output**: Approval status + feedback
- **LLM Choice**: Model that follows instructions religiously (GPT-4o-mini)

#### 3. **Optimizer** ğŸ”§
- **Role**: Improves rejected tweets
- **Input**: Original tweet + feedback
- **Output**: Improved tweet
- **LLM Choice**: Model with good rewriting capabilities (GPT-4o)

### Workflow Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GENERATE   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   (Node 1)  â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â”‚
       â”‚                   â”‚
       â–¼                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  EVALUATE    â”‚           â”‚
â”‚   (Node 2)   â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
       â”‚                   â”‚
       â–¼                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”               â”‚
   â”‚Decisionâ”‚               â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜               â”‚
       â”‚                   â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”              â”‚
   â”‚        â”‚              â”‚
Approved  Needs            â”‚
   â”‚    Improvement        â”‚
   â”‚        â”‚              â”‚
   â–¼        â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ END â”‚  â”‚ OPTIMIZE â”‚â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”˜  â”‚ (Node 3) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Logic

**Evaluation outcomes:**
1. **Approved** â†’ End workflow (or human review in production)
2. **Needs Improvement** â†’ Send to Optimizer â†’ Loop back to Evaluator

**Loop termination conditions:**
- Tweet gets approved
- Maximum iterations reached (safety mechanism)

---

## ğŸ’» Implementation Guide

### Setting Up LLMs

```python
from langchain_openai import ChatOpenAI

# Generator LLM - Strong writing capabilities
generator_llm = ChatOpenAI(model="gpt-4o")

# Evaluator LLM - Follows instructions strictly
evaluator_llm = ChatOpenAI(model="gpt-4o-mini")

# Optimizer LLM - Good at rewriting
optimizer_llm = ChatOpenAI(model="gpt-4o")
```

**Why different LLMs?**
- **Generator**: Needs creativity and writing skills â†’ Use powerful model
- **Evaluator**: Needs to follow strict criteria â†’ Use reliable, cost-effective model
- **Optimizer**: Needs to improve based on feedback â†’ Use capable model

**Production tip**: Research and select models that excel at each specific task. Different providers (OpenAI, Anthropic, Google) have different strengths.

---

### Defining State

State management is crucial in LangGraph. It tracks all variables throughout the workflow.

```python
from typing_extensions import TypedDict
from typing import Annotated, Literal
import operator

class TweetState(TypedDict):
    # User input
    topic: str
    
    # Generated content
    tweet: str
    
    # Evaluation results
    evaluation: Literal["approved", "needs_improvement"]
    feedback: str
    
    # Loop tracking
    iteration: int
    max_iteration: int
    
    # History tracking (with reducer)
    tweet_history: Annotated[list[str], operator.add]
    feedback_history: Annotated[list[str], operator.add]
```

**Breaking down each field:**

| Field | Type | Purpose | Example Value |
|-------|------|---------|---------------|
| `topic` | `str` | Topic for tweet generation | "Indian Railways" |
| `tweet` | `str` | Current generated tweet | "Boarding an Indian train..." |
| `evaluation` | `Literal` | Approval status | "approved" or "needs_improvement" |
| `feedback` | `str` | Evaluator's feedback | "Too generic, add humor" |
| `iteration` | `int` | Current loop count | 3 |
| `max_iteration` | `int` | Loop limit (safety) | 5 |
| `tweet_history` | `list[str]` | All generated tweets | ["tweet1", "tweet2", "tweet3"] |
| `feedback_history` | `list[str]` | All feedback received | ["feedback1", "feedback2"] |

**Understanding Annotated with operator.add:**

```python
tweet_history: Annotated[list[str], operator.add]
```

- **Without reducer**: Each new value replaces the old one
- **With `operator.add`**: New values are appended to the list
- **Why needed**: To maintain complete history of all iterations

---

### Building Nodes

#### Node 1: Generate Tweet

```python
from langchain_core.messages import SystemMessage, HumanMessage

def generate_tweet(state: TweetState):
    """
    Generates a funny and original tweet on the given topic.
    
    Args:
        state: Current workflow state containing topic
        
    Returns:
        Dictionary with generated tweet
    """
    
    # Create prompt with system and human messages
    messages = [
        SystemMessage(
            content="You are a funny and clever Twitter influencer."
        ),
        HumanMessage(
            content=f"""Write a short, original, and hilarious tweet on the topic: {state['topic']}

Rules:
- Do NOT use question-answer format
- Maximum 280 characters
- Use observational humor, irony, sarcasm, and cultural references
- Think in meme logic, punchlines, and relatable takes
- Use simple day-to-day English
"""
        )
    ]
    
    # Invoke generator LLM
    response = generator_llm.invoke(messages).content
    
    # Return updated state
    return {
        "tweet": response,
        "tweet_history": [response]  # Add to history
    }
```

**Line-by-line explanation:**

1. **Function signature**: Takes `TweetState`, returns dictionary
2. **SystemMessage**: Sets LLM's role/personality
3. **HumanMessage**: Provides specific instructions and topic
4. **Rules section**: Defines constraints and style guidelines
5. **LLM invocation**: Sends messages to generator model
6. **`.content`**: Extracts text from response object
7. **Return dictionary**: Updates state with new tweet
8. **List format**: `[response]` needed for reducer to work

---

#### Node 2: Evaluate Tweet

```python
from pydantic import BaseModel, Field

# Define structured output schema
class TweetEvaluationSchema(BaseModel):
    """Schema for tweet evaluation results."""
    
    evaluation: Literal["approved", "needs_improvement"] = Field(
        description="Whether the tweet is approved or needs improvement"
    )
    feedback: str = Field(
        description="Feedback for the tweet"
    )

# Create structured evaluator
structured_evaluator_llm = evaluator_llm.with_structured_output(
    TweetEvaluationSchema
)

def evaluate_tweet(state: TweetState):
    """
    Evaluates tweet against quality criteria.
    
    Args:
        state: Current workflow state containing generated tweet
        
    Returns:
        Dictionary with evaluation result and feedback
    """
    
    messages = [
        SystemMessage(
            content="""You are a ruthless, no-laugh-given Twitter critic.
You evaluate tweets based on humor, originality, virality, and tweet format."""
        ),
        HumanMessage(
            content=f"""Evaluate the following tweet: {state['tweet']}

Criteria:
1. Originality - Is it fresh? Or have we seen this 100 times?
2. Humor - Does it make you laugh?
3. Punchiness - Is there a clear punchline?
4. Virality potential - Would people share this?
5. Format - Is it well-structured?

Auto-reject if:
- Question-answer format
- Over 280 characters
- Traditional jokes

Respond in structured format:
- evaluation: "approved" or "needs_improvement"
- feedback: One paragraph explaining strengths and weaknesses
"""
        )
    ]
    
    # Get structured response
    response = structured_evaluator_llm.invoke(messages)
    
    # Return evaluation and feedback
    return {
        "evaluation": response.evaluation,
        "feedback": response.feedback,
        "feedback_history": [response.feedback]
    }
```

**Key concepts:**

**Structured Output with Pydantic:**
- **Why**: Ensures consistent response format
- **How**: Define schema using Pydantic BaseModel
- **Benefit**: Type-safe, predictable outputs

**Evaluation Criteria:**
The more detailed your criteria, the better the results:
1. **Originality**: Prevents generic content
2. **Humor**: Ensures entertainment value
3. **Punchiness**: Checks for clear message
4. **Virality**: Assesses shareability
5. **Format**: Validates structure

**Auto-reject rules:**
Hard constraints that immediately fail tweets:
- Question-answer format (overused)
- Character limit violations
- Traditional joke structures

---

#### Node 3: Optimize Tweet

```python
def optimize_tweet(state: TweetState):
    """
    Improves tweet based on evaluator feedback.
    
    Args:
        state: Current workflow state with tweet and feedback
        
    Returns:
        Dictionary with optimized tweet and incremented iteration
    """
    
    messages = [
        SystemMessage(
            content="""You punch up tweets for virality and humor based on given feedback."""
        ),
        HumanMessage(
            content=f"""Improve the tweet based on this feedback.

Feedback: {state['feedback']}
Topic: {state['topic']}
Original tweet: {state['tweet']}

Rewrite it as a short, viral-worthy tweet.
Avoid Q&A style and stay under 280 characters.
"""
        )
    ]
    
    # Generate improved tweet
    response = optimizer_llm.invoke(messages).content
    
    # Increment iteration counter
    iteration = state['iteration'] + 1
    
    # Return updated state
    return {
        "tweet": response,
        "iteration": iteration,
        "tweet_history": [response]
    }
```

**Important points:**

1. **Feedback incorporation**: Uses evaluator's feedback to guide improvement
2. **Context preservation**: Includes original tweet and topic
3. **Iteration tracking**: Increments counter for loop control
4. **History maintenance**: Adds new tweet to history

---

### Creating Edges and Loops

```python
from langgraph.graph import StateGraph

# Initialize graph
graph = StateGraph(TweetState)

# Add nodes
graph.add_node("generate", generate_tweet)
graph.add_node("evaluate", evaluate_tweet)
graph.add_node("optimize", optimize_tweet)

# Add edges
graph.add_edge("__start__", "generate")  # Start â†’ Generate
graph.add_edge("generate", "evaluate")    # Generate â†’ Evaluate

# Conditional edge with routing function
def route_evaluation(state: TweetState):
    """
    Routes based on evaluation result and iteration limit.
    
    Returns:
        "approved" - End workflow
        "needs_improvement" - Continue to optimizer
    """
    if (state['evaluation'] == "approved" or